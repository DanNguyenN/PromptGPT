{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all the Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Google Colab\n"
     ]
    }
   ],
   "source": [
    "# Check if the copmuter is on google colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    !pip install rich\n",
    "    !pip install -q -U bitsandbytes\n",
    "    !pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "    !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "    !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "    !pip install datasets\n",
    "    !pip install wandb\n",
    "    !pip install ray[tune]\n",
    "    !pip install langchain\n",
    "    !pip install session-info\n",
    "    !pip install scipy\n",
    "else:\n",
    "    print(\"Not running on Google Colab\")\n",
    "from rich import print\n",
    "import logging\n",
    "from pathlib import Path\n",
    "logger = logging.getLogger(__name__)\n",
    "#ROOT_PATH = Path(__file__).parent.parent\n",
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the GPU env\n",
    "1. You can check the GPU in the Google Colab by clicking  and efficieny\n",
    "2. Check if the GPU can use bfloat16 most effective as most model are pre-trained with bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from rich import print\n",
    "if torch.cuda.is_available():\n",
    "    !nvidia-smi\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"Cuda capability: \", torch.cuda.get_device_capability(0))\n",
    "    '''\n",
    "    On pre-ampere hardware bf16 works, but doesn't provide speed-ups compared to fp32 matmul operations, and some matmul operations are failing outright, so this check is more like \"guaranteed to work and be performant\" than \"works somehow\".  https://github.com/pytorch/pytorch/issues/75427\n",
    "    '''\n",
    "    print(f\"bfloat16 support: { torch.cuda.is_bf16_supported()}\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Seed Environment of the Notebook to ensure the reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "set_seed( DEFAULT_SEED )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Tokenizers\n",
    "1. We are suing Dolly model which was trained on the Pythia model. Instead we are recreating the dollvy tokenizer from the Pythia tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTNeoXTokenizerFast</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name_or_path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'databricks/dolly-v2-7b'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50254</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">model_max_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000000000000000019884624838656</span>, <span style=\"color: #808000; text-decoration-color: #808000\">is_fast</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">padding_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">truncation_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">special_tokens</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'bos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'eos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'unk_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'pad_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'additional_special_tokens'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'### End'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'### Instruction:'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'### Response:'</span><span style=\"font-weight: bold\">]}</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">clean_up_tokenization_spaces</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mGPTNeoXTokenizerFast\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname_or_path\u001b[0m=\u001b[32m'databricks/dolly-v2-7b'\u001b[0m, \u001b[33mvocab_size\u001b[0m=\u001b[1;36m50254\u001b[0m, \n",
       "\u001b[33mmodel_max_length\u001b[0m=\u001b[1;36m1000000000000000019884624838656\u001b[0m, \u001b[33mis_fast\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mpadding_side\u001b[0m=\u001b[32m'right'\u001b[0m, \u001b[33mtruncation_side\u001b[0m=\u001b[32m'right'\u001b[0m, \n",
       "\u001b[33mspecial_tokens\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'bos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'eos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'unk_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \n",
       "\u001b[32m'pad_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'additional_special_tokens'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'### End'\u001b[0m, \u001b[32m'### Instruction:'\u001b[0m, \u001b[32m'### Response:'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m, \n",
       "\u001b[33mclean_up_tokenization_spaces\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTNeoXTokenizerFast</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name_or_path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'EleutherAI/pythia-6.9b'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50254</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">model_max_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000000000000000019884624838656</span>, <span style=\"color: #808000; text-decoration-color: #808000\">is_fast</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">padding_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">truncation_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">special_tokens</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'bos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'eos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'unk_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span><span style=\"font-weight: bold\">}</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">clean_up_tokenization_spaces</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mGPTNeoXTokenizerFast\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname_or_path\u001b[0m=\u001b[32m'EleutherAI/pythia-6.9b'\u001b[0m, \u001b[33mvocab_size\u001b[0m=\u001b[1;36m50254\u001b[0m, \n",
       "\u001b[33mmodel_max_length\u001b[0m=\u001b[1;36m1000000000000000019884624838656\u001b[0m, \u001b[33mis_fast\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mpadding_side\u001b[0m=\u001b[32m'right'\u001b[0m, \u001b[33mtruncation_side\u001b[0m=\u001b[32m'right'\u001b[0m, \n",
       "\u001b[33mspecial_tokens\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'bos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'eos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'unk_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m\u001b[1m}\u001b[0m, \n",
       "\u001b[33mclean_up_tokenization_spaces\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTNeoXTokenizerFast</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name_or_path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'EleutherAI/pythia-6.9b'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50254</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">model_max_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000000000000000019884624838656</span>, <span style=\"color: #808000; text-decoration-color: #808000\">is_fast</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">padding_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">truncation_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">special_tokens</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'bos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'eos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'unk_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'pad_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'additional_special_tokens'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'### End'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'### Instruction:'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'### Response:'</span><span style=\"font-weight: bold\">]}</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">clean_up_tokenization_spaces</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mGPTNeoXTokenizerFast\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname_or_path\u001b[0m=\u001b[32m'EleutherAI/pythia-6.9b'\u001b[0m, \u001b[33mvocab_size\u001b[0m=\u001b[1;36m50254\u001b[0m, \n",
       "\u001b[33mmodel_max_length\u001b[0m=\u001b[1;36m1000000000000000019884624838656\u001b[0m, \u001b[33mis_fast\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mpadding_side\u001b[0m=\u001b[32m'right'\u001b[0m, \u001b[33mtruncation_side\u001b[0m=\u001b[32m'right'\u001b[0m, \n",
       "\u001b[33mspecial_tokens\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'bos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'eos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'unk_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \n",
       "\u001b[32m'pad_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'additional_special_tokens'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'### End'\u001b[0m, \u001b[32m'### Instruction:'\u001b[0m, \u001b[32m'### Response:'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m, \n",
       "\u001b[33mclean_up_tokenization_spaces\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Special Tokens\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "INPUT_KEY = \"Input:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "RESPONSE_KEY_NL = f\"{RESPONSE_KEY}\\n\"\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"databricks/dolly-v2-3b\"#\"databricks/dolly-v2-3b\"\n",
    "eleutherai_python_3b = \"EleutherAI/pythia-2.8b\"\n",
    "eleutherai_python_7b = \"EleutherAI/pythia-6.9b\"\n",
    "dolly_v2_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "print(dolly_v2_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Model\n",
    "1. Torch Datat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Bits and Butes Config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Bit Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_63870/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2926075251.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">12</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_63870/2926075251.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/null/miniconda3/envs/temoctalk/lib/python3.10/site-packages/torch/cuda/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">122</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">is_bf16_supported</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 119 │   │   </span>cuda_maj_decide = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>(cu_vers.split(<span style=\"color: #808000; text-decoration-color: #808000\">'.'</span>)[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]) &gt;= <span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 120 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 121 │   │   </span>cuda_maj_decide = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 122 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch.cuda.get_device_properties(torch.cuda.current_device()).major &gt;= <span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 123 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 124 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_sleep</span>(cycles):                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 125 │   </span>torch._C._cuda_sleep(cycles)                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/null/miniconda3/envs/temoctalk/lib/python3.10/site-packages/torch/cuda/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">674</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">current_device</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 671 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 672 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">current_device</span>() -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>:                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 673 │   </span><span style=\"color: #808000; text-decoration-color: #808000\">r\"\"\"Returns the index of a currently selected device.\"\"\"</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 674 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>_lazy_init()                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 675 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch._C._cuda_getDevice()                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 676 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 677 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/null/miniconda3/envs/temoctalk/lib/python3.10/site-packages/torch/cuda/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">247</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_lazy_init</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 244 │   │   # are found or any other error occurs</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 245 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">'CUDA_MODULE_LOADING'</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> os.environ:                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 246 │   │   │   </span>os.environ[<span style=\"color: #808000; text-decoration-color: #808000\">'CUDA_MODULE_LOADING'</span>] = <span style=\"color: #808000; text-decoration-color: #808000\">'LAZY'</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 247 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch._C._cuda_init()                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 248 │   │   # Some of the queued calls may reentrantly call _lazy_init();</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 249 │   │   # we need to just return without initializing in that case.</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 250 │   │   # However, we must not let any *other* threads in!</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a \n",
       "driver from <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.nvidia.com/Download/index.aspx</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_63870/\u001b[0m\u001b[1;33m2926075251.py\u001b[0m:\u001b[94m12\u001b[0m in \u001b[92m<module>\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_63870/2926075251.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/null/miniconda3/envs/temoctalk/lib/python3.10/site-packages/torch/cuda/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m122\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mis_bf16_supported\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 119 \u001b[0m\u001b[2m│   │   \u001b[0mcuda_maj_decide = \u001b[96mint\u001b[0m(cu_vers.split(\u001b[33m'\u001b[0m\u001b[33m.\u001b[0m\u001b[33m'\u001b[0m)[\u001b[94m0\u001b[0m]) >= \u001b[94m11\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 120 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 121 \u001b[0m\u001b[2m│   │   \u001b[0mcuda_maj_decide = \u001b[94mFalse\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 122 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m torch.cuda.get_device_properties(torch.cuda.current_device()).major >= \u001b[94m8\u001b[0m \u001b[95mand\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 123 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 124 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_sleep\u001b[0m(cycles):                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 125 \u001b[0m\u001b[2m│   \u001b[0mtorch._C._cuda_sleep(cycles)                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/null/miniconda3/envs/temoctalk/lib/python3.10/site-packages/torch/cuda/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m674\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mcurrent_device\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 671 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 672 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcurrent_device\u001b[0m() -> \u001b[96mint\u001b[0m:                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 673 \u001b[0m\u001b[2m│   \u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 674 \u001b[2m│   \u001b[0m_lazy_init()                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 675 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m torch._C._cuda_getDevice()                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 676 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 677 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/null/miniconda3/envs/temoctalk/lib/python3.10/site-packages/torch/cuda/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m247\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_lazy_init\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 244 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# are found or any other error occurs\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 245 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m'\u001b[0m\u001b[33mCUDA_MODULE_LOADING\u001b[0m\u001b[33m'\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m os.environ:                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 246 \u001b[0m\u001b[2m│   │   │   \u001b[0mos.environ[\u001b[33m'\u001b[0m\u001b[33mCUDA_MODULE_LOADING\u001b[0m\u001b[33m'\u001b[0m] = \u001b[33m'\u001b[0m\u001b[33mLAZY\u001b[0m\u001b[33m'\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 247 \u001b[2m│   │   \u001b[0mtorch._C._cuda_init()                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 248 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 249 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# we need to just return without initializing in that case.\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 250 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# However, we must not let any *other* threads in!\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mFound no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a \n",
       "driver from \u001b[4;94mhttp://www.nvidia.com/Download/index.aspx\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neox-20b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit  = True,\n",
    "    llm_int8_threshold = 6.0,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the LM Models\n",
    "Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_63870/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2576624906.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_63870/2576624906.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AssertionError: </span>You need to have a GPU to run this notebook.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_63870/\u001b[0m\u001b[1;33m2576624906.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_63870/2576624906.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAssertionError: \u001b[0mYou need to have a GPU to run this notebook.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "assert torch.cuda.is_available(), \"You need to have a GPU to run this notebook.\"\n",
    "n_gpus = torch.cuda.device_count()\n",
    "def model_init():\n",
    "    free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "    max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {n_gpus}\")\n",
    "    max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "    print(f\"Max memory: {max_memory}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path = PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "        trust_remote_code = True,\n",
    "        use_cache = False,\n",
    "        torch_dtype =  torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        device_map = \"auto\",\n",
    "        load_in_8bit = False,\n",
    "        load_in_4bit = True,\n",
    "        low_cpu_mem_usage = True, # low cpu memory usage is to be true when the device map is auto\n",
    "        max_memory =  max_memory,\n",
    "        quantization_config = bnb_config,\n",
    "    )\n",
    "    model.eval() # set the model to eval mode during inference\n",
    "    return model\n",
    "\n",
    "model = model_init()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation COnfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, GenerationConfig\n",
    "import random\n",
    "generation_config =  GenerationConfig(\n",
    "    max_new_tokens = 256, # The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "    num_beams = 1, # 1 means no beam search instead greedy search\n",
    "    temperature = random.uniform(0.01 , .98), # Parameters for manipulation of the model output logits\n",
    "    top_p = 0.92, # Parameters for manipulation of the model output logits\n",
    "    top_k = 50, # Parameters to only select the top-k tokens, instead of sampling from the distribution\n",
    "    do_sample = True ,# select a random token from the top-k tokens (set to 0 to disable top-k sampling) instead of choosing the one with the highest probability\n",
    "    use_cache = True, # Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
    "    repetition_penalty = 1.02, # The parameter for repetition penalty. 1.0 means no penalty. See this paper for more details.\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the adaper config if not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f77c96c531444be8a9e230614ab63fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/adapter_config.json:   0%|          | 0.00/470 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "repo_name = \"Rami/dolly_prompt_generator\"\n",
    "config = PeftConfig.from_pretrained(repo_name) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Model and Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_63870/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">92192561.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">6</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_63870/92192561.py'</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span> is not defined\n",
       "\n",
       "<span style=\"font-style: italic\">During handling of the above exception, another exception occurred:</span>\n",
       "\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_63870/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">92192561.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_63870/92192561.py'</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'model_init'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_63870/\u001b[0m\u001b[1;33m92192561.py\u001b[0m:\u001b[94m6\u001b[0m in \u001b[92m<module>\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_63870/92192561.py'\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'model'\u001b[0m is not defined\n",
       "\n",
       "\u001b[3mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
       "\n",
       "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_63870/\u001b[0m\u001b[1;33m92192561.py\u001b[0m:\u001b[94m11\u001b[0m in \u001b[92m<module>\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_63870/92192561.py'\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'model_init'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "inference_model = None\n",
    "try:\n",
    "    inference_model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        repo_name,\n",
    "    )\n",
    "except NameError as e:\n",
    "    ## Donwload the model from the HFhub\n",
    "    model = model_init()\n",
    "    \n",
    "    inference_model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        repo_name,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Instruction Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from transformers import Pipeline, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "INTRO_BLURB = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    ")\n",
    "\n",
    "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
    "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "\n",
    "def get_special_token_id(tokenizer: PreTrainedTokenizer, key: str) -> int:\n",
    "    \"\"\"Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
    "    When training, we configure the tokenizer so that the sequences like \"### Instruction:\" and \"### End\" are\n",
    "    treated specially and converted to a single, new token.  This retrieves the token ID each of these keys map to.\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
    "        key (str): the key to convert to a single token\n",
    "    Raises:\n",
    "        RuntimeError: if more than one ID was generated\n",
    "    Returns:\n",
    "        int: the token ID for the given key\n",
    "    \"\"\"\n",
    "    token_ids = tokenizer.encode(key)\n",
    "    if len(token_ids) > 1:\n",
    "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
    "    return token_ids[0]\n",
    "\n",
    "from transformers import AutoModelForCausalLM, GenerationConfig\n",
    "class InstructionTextGenerationPipeline(Pipeline):\n",
    "    def __init__(\n",
    "        self, \n",
    "        generation_config: GenerationConfig = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Initialize the pipeline\n",
    "        Args:\n",
    "            do_sample (bool, optional): Whether or not to use sampling. Defaults to True.\n",
    "            max_new_tokens (int, optional): Max new tokens after the prompt to generate. Defaults to 128.\n",
    "            top_p (float, optional): If set to float < 1, only the smallest set of most probable tokens with\n",
    "                probabilities that add up to top_p or higher are kept for generation. Defaults to 0.92.\n",
    "            top_k (int, optional): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "                Defaults to 0.\n",
    "        \"\"\"\n",
    "        self.generation_config: GenerationConfig = generation_config\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _sanitize_parameters(self,\n",
    "                             return_full_text: bool = None,\n",
    "                             **generate_kwargs):\n",
    "        preprocess_params = {}\n",
    "        assert self.generation_config is not None, \"Generation config is not initialized.\"\n",
    "\n",
    "        # newer versions of the tokenizer configure the response key as a special token.  newer versions still may\n",
    "        # append a newline to yield a single token.  find whatever token is configured for the response key.\n",
    "        tokenizer_response_key = next(\n",
    "            (token for token in self.tokenizer.additional_special_tokens if token.startswith(RESPONSE_KEY)), None\n",
    "        )\n",
    "\n",
    "        response_key_token_id = None\n",
    "        end_key_token_id = None\n",
    "        if tokenizer_response_key:\n",
    "            try:\n",
    "                response_key_token_id = get_special_token_id(self.tokenizer, tokenizer_response_key)\n",
    "                end_key_token_id = get_special_token_id(self.tokenizer, END_KEY)\n",
    "\n",
    "                # Ensure generation stops once it generates \"### End\"\n",
    "                generate_kwargs[\"eos_token_id\"] = end_key_token_id\n",
    "                self.generation_config.eos_token_id = end_key_token_id\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        forward_params = generate_kwargs\n",
    "        postprocess_params = {\n",
    "            \"response_key_token_id\": response_key_token_id,\n",
    "            \"end_key_token_id\": end_key_token_id\n",
    "        }\n",
    "\n",
    "        if return_full_text is not None:\n",
    "            postprocess_params[\"return_full_text\"] = return_full_text\n",
    "            print(postprocess_params)\n",
    "\n",
    "        return preprocess_params, forward_params, postprocess_params\n",
    "\n",
    "    def preprocess(self, instruction_text, **generate_kwargs):\n",
    "        prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n",
    "        inputs = self.tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs[\"prompt_text\"] = prompt_text\n",
    "        inputs[\"instruction_text\"] = instruction_text\n",
    "        return inputs\n",
    "    ## Only Once\n",
    "    def _forward(self, model_inputs , eos_token_id):\n",
    "        assert self.model is not None, \"Model is not initialized.\"\n",
    "        assert self.generation_config is not None, \"Generation config is not initialized.\"\n",
    "        assert self.tokenizer is not None, \"Tokenizer is not initialized.\"\n",
    "        assert self.tokenizer.pad_token_id is not None, \"Tokenizer does not have a pad token ID.\"\n",
    "        print(self.generation_config)\n",
    "        input_ids = model_inputs[\"input_ids\"]\n",
    "        attention_mask = model_inputs.get(\"attention_mask\", None)\n",
    "\n",
    "        if input_ids.shape[1] == 0:\n",
    "            input_ids = None\n",
    "            attention_mask = None\n",
    "            in_b = 1\n",
    "        else:\n",
    "            in_b = input_ids.shape[0]\n",
    "\n",
    "        generated_sequence = self.model.generate(\n",
    "            input_ids=input_ids.to(self.model.device),\n",
    "            attention_mask=attention_mask.to(self.model.device) if attention_mask is not None else None,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            generation_config=self.generation_config,\n",
    "        )\n",
    "\n",
    "        out_b = generated_sequence.shape[0]\n",
    "        if self.framework == \"pt\":\n",
    "            generated_sequence = generated_sequence.reshape(in_b, out_b // in_b, *generated_sequence.shape[1:])\n",
    "        elif self.framework == \"tf\":\n",
    "            generated_sequence = tf.reshape(generated_sequence, (in_b, out_b // in_b, *generated_sequence.shape[1:]))\n",
    "\n",
    "        instruction_text = model_inputs.pop(\"instruction_text\")\n",
    "        return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"instruction_text\": instruction_text}\n",
    "\n",
    "    def postprocess(self, model_outputs, response_key_token_id, end_key_token_id, return_full_text: bool = False):\n",
    "\n",
    "        generated_sequence = model_outputs[\"generated_sequence\"][0]\n",
    "        instruction_text = model_outputs[\"instruction_text\"]\n",
    "\n",
    "        generated_sequence: List[List[int]] = generated_sequence.numpy().tolist()\n",
    "        records = []\n",
    "        for sequence in generated_sequence:\n",
    "\n",
    "            # The response will be set to this variable if we can identify it.\n",
    "            decoded = None\n",
    "\n",
    "            # If we have token IDs for the response and end, then we can find the tokens and only decode between them.\n",
    "            if response_key_token_id and end_key_token_id:\n",
    "                # Find where \"### Response:\" is first found in the generated tokens.  Considering this is part of the\n",
    "                # prompt, we should definitely find it.  We will return the tokens found after this token.\n",
    "                try:\n",
    "                    response_pos = sequence.index(response_key_token_id)\n",
    "                except ValueError:\n",
    "                    logger.warn(f\"Could not find response key {response_key_token_id} in: {sequence}\")\n",
    "                    response_pos = None\n",
    "\n",
    "                if response_pos:\n",
    "                    # Next find where \"### End\" is located.  The model has been trained to end its responses with this\n",
    "                    # sequence (or actually, the token ID it maps to, since it is a special token).  We may not find\n",
    "                    # this token, as the response could be truncated.  If we don't find it then just return everything\n",
    "                    # to the end.  Note that even though we set eos_token_id, we still see the this token at the end.\n",
    "                    try:\n",
    "                        end_pos = sequence.index(end_key_token_id)\n",
    "                    except ValueError:\n",
    "                        end_pos = None\n",
    "\n",
    "                    decoded = self.tokenizer.decode(sequence[response_pos + 1 : end_pos]).strip()\n",
    "\n",
    "            if not decoded:\n",
    "                # Otherwise we'll decode everything and use a regex to find the response and end.\n",
    "\n",
    "                fully_decoded = self.tokenizer.decode(sequence)\n",
    "\n",
    "                # The response appears after \"### Response:\".  The model has been trained to append \"### End\" at the\n",
    "                # end.\n",
    "                m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", fully_decoded, flags=re.DOTALL)\n",
    "\n",
    "                if m:\n",
    "                    decoded = m.group(1).strip()\n",
    "                else:\n",
    "                    # The model might not generate the \"### End\" sequence before reaching the max tokens.  In this case,\n",
    "                    # return everything after \"### Response:\".\n",
    "                    m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded, flags=re.DOTALL)\n",
    "                    if m:\n",
    "                        decoded = m.group(1).strip()\n",
    "                    else:\n",
    "                        logger.warn(f\"Failed to find response in:\\n{fully_decoded}\")\n",
    "\n",
    "            # If the full text is requested, then append the decoded text to the original instruction.\n",
    "            # This technically isn't the full text, as we format the instruction in the prompt the model has been\n",
    "            # trained on, but to the client it will appear to be the full text.\n",
    "            if return_full_text:\n",
    "                decoded = f\"{instruction_text}\\n{decoded}\"\n",
    "\n",
    "            rec = {\"generated_text\": decoded}\n",
    "\n",
    "            records.append(rec)\n",
    "\n",
    "        return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'response_key_token_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50279</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'end_key_token_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50277</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'return_full_text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'response_key_token_id'\u001b[0m: \u001b[1;36m50279\u001b[0m, \u001b[32m'end_key_token_id'\u001b[0m: \u001b[1;36m50277\u001b[0m, \u001b[32m'return_full_text'\u001b[0m: \u001b[3;92mTrue\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">text-generation\n",
       "</pre>\n"
      ],
      "text/plain": [
       "text-generation\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">({}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'eos_token_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50277</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'response_key_token_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50279</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'end_key_token_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50277</span><span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'eos_token_id'\u001b[0m: \u001b[1;36m50277\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'response_key_token_id'\u001b[0m: \u001b[1;36m50279\u001b[0m, \u001b[32m'end_key_token_id'\u001b[0m: \u001b[1;36m50277\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">GenerationConfig <span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"do_sample\"</span>: true,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"eos_token_id\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50277</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"max_new_tokens\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"repetition_penalty\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.02</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"temperature\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.724377077739092</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"top_p\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.92</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"transformers_version\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"4.29.2\"</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "GenerationConfig \u001b[1m{\u001b[0m\n",
       "  \u001b[32m\"do_sample\"\u001b[0m: true,\n",
       "  \u001b[32m\"eos_token_id\"\u001b[0m: \u001b[1;36m50277\u001b[0m,\n",
       "  \u001b[32m\"max_new_tokens\"\u001b[0m: \u001b[1;36m256\u001b[0m,\n",
       "  \u001b[32m\"repetition_penalty\"\u001b[0m: \u001b[1;36m1.02\u001b[0m,\n",
       "  \u001b[32m\"temperature\"\u001b[0m: \u001b[1;36m0.724377077739092\u001b[0m,\n",
       "  \u001b[32m\"top_p\"\u001b[0m: \u001b[1;36m0.92\u001b[0m,\n",
       "  \u001b[32m\"transformers_version\"\u001b[0m: \u001b[32m\"4.29.2\"\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'### Instruction: What is the capital of France? ### Response:\\n### Response:\\nIt is a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">very important task to note that the capital of France is a\\ncapital of France. The capital of France is the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capital of France, the\\ncapital of France is the capital of France. The capital of France is the capital </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of\\nFrance. The capital of France is the capital of France. The capital of France is\\nthe capital of France. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capital of France is the capital of France. The capital\\nof France is the capital of France. The capital of France </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is the capital of\\nFrance. The capital of France is the capital of France. The capital of\\nFrance is the capital of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">France. The capital of France is the capital of\\nFrance. The capital of France is the capital of France. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capital of\\nFrance is the capital of France. The capital of France is the capital of\\nFrance. The capital of France</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is the capital aids in the capital of France. The capital of\\nFrance is the capital of France. The capital of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">France is the capital of\\nFrance. The capital of France is the capital of France. The capital of France is the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capital of\\nFrance. The capital of France is the capital of France. The capital of France is the capital </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of\\nFrance. The capital of France is the capital of France'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'generated_text'\u001b[0m: \u001b[32m'### Instruction: What is the capital of France? ### Response:\\n### Response:\\nIt is a \u001b[0m\n",
       "\u001b[32mvery important task to note that the capital of France is a\\ncapital of France. The capital of France is the \u001b[0m\n",
       "\u001b[32mcapital of France, the\\ncapital of France is the capital of France. The capital of France is the capital \u001b[0m\n",
       "\u001b[32mof\\nFrance. The capital of France is the capital of France. The capital of France is\\nthe capital of France. The \u001b[0m\n",
       "\u001b[32mcapital of France is the capital of France. The capital\\nof France is the capital of France. The capital of France \u001b[0m\n",
       "\u001b[32mis the capital of\\nFrance. The capital of France is the capital of France. The capital of\\nFrance is the capital of\u001b[0m\n",
       "\u001b[32mFrance. The capital of France is the capital of\\nFrance. The capital of France is the capital of France. The \u001b[0m\n",
       "\u001b[32mcapital of\\nFrance is the capital of France. The capital of France is the capital of\\nFrance. The capital of France\u001b[0m\n",
       "\u001b[32mis the capital aids in the capital of France. The capital of\\nFrance is the capital of France. The capital of \u001b[0m\n",
       "\u001b[32mFrance is the capital of\\nFrance. The capital of France is the capital of France. The capital of France is the \u001b[0m\n",
       "\u001b[32mcapital of\\nFrance. The capital of France is the capital of France. The capital of France is the capital \u001b[0m\n",
       "\u001b[32mof\\nFrance. The capital of France is the capital of France'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "generate_text = InstructionTextGenerationPipeline(model=model,\n",
    "                                                  tokenizer = dolly_v2_tokenizer, \n",
    "                                                  task=\"text-generation\" , \n",
    "                                                  return_full_text=True,\n",
    "                                                  generation_config=generation_config)\n",
    "                                                  \n",
    "print(generate_text.task)\n",
    "print(generate_text._sanitize_parameters())\n",
    "print(generate_text(\"### Instruction: What is the capital of France? ### Response:\"))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Prompt with Hugging Face Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# template for an instrution with no input\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\"],\n",
    "    template=\"{instruction}\")\n",
    "\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n",
    "\n",
    "\n",
    "\n",
    "print(llm_chain.predict(instruction=\"Explain to me the difference between nuclear fission and fusion.\").lstrip())\n",
    "\n",
    "context = \"\"\"George Washington (February 22, 1732[b] – December 14, 1799) was an American military officer, statesman,\n",
    "and Founding Father who served as the first president of the United States from 1789 to 1797.\"\"\"\n",
    "\n",
    "print(llm_context_chain.predict(instruction=\"When was George Washington president?\", context=context).lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT_INSTRUCTION = \"Given a prompt from the user that was meant to be feed into a GPT style model , rewrite the prompt that will improve the quality of the generated text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    llm_context_chain.predict(\n",
    "        instruction = GENERATE_PROMPT_INSTRUCTION,\n",
    "        context=\"A product description on an E-commerce website\")\n",
    "    .lstrip()\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_int():\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] [Dolly Github](https://github.com/databrickslabs/dolly/blob/5021d941d95dddcf1f00d978d7f944709873f419/training/trainer.py#L138)\n",
    "[2] https://gist.github.com/Birch-san/57878c4a27cf34f57d3e861865a7d0a2\n",
    "[3] https://github.com/artidoro/qlora/blob/main/qlora.py \n",
    "[4] https://github.com/tloen/alpaca-lora/blob/main/finetune.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temoctalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
