{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 MosaicML LLM Foundry authors\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "\"\"\"Basic HuggingFace -> ONNX export script.\n",
    "\n",
    "This scripts show a basic HuggingFace -> ONNX export workflow. This works for a MPT model\n",
    "that has been saved using `MPT.save_pretrained`. For more details and examples\n",
    "of exporting and working with HuggingFace models with ONNX, see https://huggingface.co/docs/transformers/serialization#export-to-onnx.\n",
    "\n",
    "Example usage:\n",
    "\n",
    "    1) Local export\n",
    "\n",
    "    python convert_hf_to_onnx.py --pretrained_model_name_or_path local/path/to/huggingface/folder --output_folder local/folder\n",
    "\n",
    "    2) Remote export\n",
    "\n",
    "    python convert_hf_to_onnx.py --pretrained_model_name_or_path local/path/to/huggingface/folder --output_folder s3://bucket/remote/folder\n",
    "\n",
    "    3) Verify the exported model\n",
    "\n",
    "    python convert_hf_to_onnx.py --pretrained_model_name_or_path local/path/to/huggingface/folder --output_folder local/folder --verify_export\n",
    "\n",
    "    4) Change the batch size or max sequence length\n",
    "\n",
    "    python convert_hf_to_onnx.py --pretrained_model_name_or_path local/path/to/huggingface/folder --output_folder local/folder --export_batch_size 1 --max_seq_len 32000\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from argparse import ArgumentTypeError\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "#from composer.utils import (maybe_create_object_store_from_uri, parse_uri, reproducibility\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Model\n",
    "1. I think it will be best to download the model 16 bits precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
