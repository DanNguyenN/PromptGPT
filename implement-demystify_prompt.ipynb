{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the Necessary Libraries\n",
    "1. Install the libraries\n",
    "\t1. Transformers\n",
    "\t2. bitsandbytes\n",
    "\t3. accelerate\n",
    "\t4. dataesets\n",
    "\t5. prompt source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Google Colab\n"
     ]
    }
   ],
   "source": [
    "# Check if the copmuter is on google colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    !pip install rich\n",
    "    !pip install \"accelerate>=0.16.0,<1\" \n",
    "    !pip install \"torch>=1.13.1\"\n",
    "    !pip install \"transformers[torch]>=4.28.1,<5\" \n",
    "    !pip install \"datasets>=1.14.0,<2\"\n",
    "    !pip install bitsandbytes\n",
    "    !pip install datasets\n",
    "    !pip install sentencepiece\n",
    "    !pip install triton\n",
    "    !pip install einops\n",
    "    !pip install safetensors\n",
    "    !pip install langchain\n",
    "    !pip install gradio\n",
    "else:\n",
    "    print(\"Not running on Google Colab\")\n",
    "from rich import print"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from rich import print\n",
    "if torch.cuda.is_available():\n",
    "    !nvidia-smi\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"Cuda capability: \", torch.cuda.get_device_capability(0))\n",
    "    '''\n",
    "    On pre-ampere hardware bf16 works, but doesn't provide speed-ups compared to fp32 matmul operations, and some matmul operations are failing outright, so this check is more like \"guaranteed to work and be performant\" than \"works somehow\".  https://github.com/pytorch/pytorch/issues/75427\n",
    "    '''\n",
    "    print(f\"bfloat16 support: { torch.cuda.is_bf16_supported()}\") # "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are selecting one nlp text classification dataset from the papers\n",
    "\t1. GLUE cola\n",
    "\t2. Newsprop\n",
    "\t3. AG New\n",
    "\t4. IMDB\n",
    "\t5. DBPEDIA\n",
    "\t6. Emotion\n",
    "\t7. Tweet offensive\n",
    "If you want to load a different dataset, you can replace `\"glue\"` and `\"cola\"` with the appropriate dataset name and split. Here are the corresponding dataset names for the options you provided:\n",
    "\n",
    "- \"GLUE cola\": `\"glue\", \"cola\"`\n",
    "- \"Newsprop\": `\"newsprop\"`\n",
    "- \"AG New\": `\"ag_news\"`\n",
    "- \"IMDB\": `\"imdb\"`\n",
    "- \"DBPEDIA\": `\"dbpedia\"`\n",
    "- \"Emotion\": `\"emotion\"`\n",
    "- \"Tweet offensive\": `\"twt_offensive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ag_news (/home/null/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a18aae2510c45f081e24e25d6084241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetDict</span><span style=\"font-weight: bold\">({</span>\n",
       "    train: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120000</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "    test: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7600</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "<span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDatasetDict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "    train: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'text'\u001b[0m, \u001b[32m'label'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m120000\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    test: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'text'\u001b[0m, \u001b[32m'label'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m7600\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Value</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'string'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ClassLabel</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">names</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'World'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Sports'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Business'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Sci/Tech'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'text'\u001b[0m: \u001b[1;35mValue\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[32m'string'\u001b[0m, \u001b[33mid\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'label'\u001b[0m: \u001b[1;35mClassLabel\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnames\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'World'\u001b[0m, \u001b[32m'Sports'\u001b[0m, \u001b[32m'Business'\u001b[0m, \u001b[32m'Sci/Tech'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mid\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from rich import print\n",
    "# Loading a text classification dataset \n",
    "dataset = load_dataset(\"ag_news\")\n",
    "print(dataset)\n",
    "print(dataset['train'].features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Seed Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_prompt = \"What label best describes this news article?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of Meta Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor index in range(0 , len(list_of_meta_prompt)):\\n    list_of_meta_prompt[index] = \"Write a paraphrase for the following sentence: What label best describes this news article? Paraphrase: Which term accurately characterizes this news article?\\n\"+list_of_meta_prompt[index]\\nprint(list_of_meta_prompt)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rich import print\n",
    "# Convert the number of items into python f string for example\n",
    "list_of_meta_prompt = [\n",
    "f\"Write a paraphrase for the following sentence: {seed_prompt} Paraphrase:\",\n",
    "f\"{seed_prompt} Paraphrase:\",\n",
    "f\"Write a likely paraphrase of the text: {seed_prompt} Paraphrase:\",\n",
    "f\"Write a similar sentence similar to the following one: {seed_prompt} Paraphrase:\",\n",
    "f\"Paraphrase the following sentence: {seed_prompt} Paraphrase:\",\n",
    "f\"Write a variation of this sentence: {seed_prompt}\",\n",
    "f\"How would you say the following sentence in a different way? {seed_prompt}\",\n",
    "]\n",
    "'''\n",
    "for index in range(0 , len(list_of_meta_prompt)):\n",
    "    list_of_meta_prompt[index] = \"Write a paraphrase for the following sentence: What label best describes this news article? Paraphrase: Which term accurately characterizes this news article?\\n\"+list_of_meta_prompt[index]\n",
    "print(list_of_meta_prompt)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Model and Tokenizers that will be paraphrased\n",
    "5. The paper recommend to use GPT3 as is impressive ability to do and to ensure there is separation between the model. I will be using multiple model long instruction model, regular instruction, and dolly\n",
    "\t1. List of model to select from\n",
    "\t\t1. Flan T5\n",
    "\t\t\t1. 11 billion or xxl which you need 88 Gigabytes in fp 32 , if load_8_bits you need bit above 16 bits.\n",
    "\t\t\t2. 3 billion xl which will be around\n",
    "\t\t2. MPT\n",
    "\t\t\t1. MPT-7b ( mosaicml/mpt-7b )\n",
    "\t\t\t\t1. Architecture\n",
    "\t\t\t\t\t1. [[ALiBi]]\n",
    "\t\t\t\t\t2. [[FasterTransformers]]\n",
    "\t\t\t\t\t3. [[Flash Attention]]\n",
    "\t2. Inference Notes on Github \n",
    "\t\t1. https://github.com/huggingface/transformers/pull/10956\n",
    "\t\t2. https://github.com/huggingface/transformers/pull/20878\n",
    "\t\t3. https://github.com/huggingface/transformers/pull/20760\n",
    "\t\t4. https://github.com/huggingface/transformers/pull/20683A\n",
    "\t\t5. https://github.com/huggingface/transformers/pull/19468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration , AutoTokenizer\n",
    "import time\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    '''\n",
    "    The Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead of 23 bits precision it has only 10 bits (same as fp16). In total it uses only 19 bits.\n",
    "    It’s magical in the sense that you can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput improvement. All you need to do is to add this to your code:\n",
    "    '''\n",
    "    if torch.cuda.get_device_name(0) in [\"A100-SXM4-40GB\", \"A100-SXM4-80GB\"]:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True \n",
    "    MODEL  = \"databricks/dolly-v2-3b\"\n",
    "    start_time_to_download_tokenizer = time.time()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    print(f\"Time to download tokenizer: {time.time() - start_time_to_download_tokenizer}\")\n",
    "    print(\"Using AutoTokenizer\")\n",
    "else:\n",
    "    MODEL  = \"databricks/dolly-v2-3b\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the 11B encoder and decoder parameter model which will take a while which will be 80 GB in fp 32\n",
    "from transformers import AutoModelForSeq2SeqLM , AutoModelForCausalLM , AutoConfig\n",
    "import time\n",
    "start_time_to_download_model = time.time()\n",
    "model = None\n",
    "if MODEL in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xxl\", \n",
    "                                                    device_map=\"auto\", # When passing a device_map, low_cpu_mem_usage is automatically set to True, so you don’t need to specify it: \n",
    "                                                    load_in_8bit=True,\n",
    "                                                    torch_dtype=torch.float16 #  due to requirements of `bitsandbytes` to enable model loading in mixed int8\n",
    "    )\n",
    "elif MODEL in  [\"mosaicml/mpt-7b\" , \"distilgpt2\" , \"databricks/dolly-v2-12b\" , \"databricks/dolly-v2-3b\"]:\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path = MODEL , \n",
    "        trust_remote_code = True # trust_remote_code=True be passed to the from_pretrained method. This is because we use a custom MPT model architecture that is not yet part of the Hugging Face transformers package. MPT includes options for many training efficiency features such as FlashAttention, ALiBi, QK LayerNorm, and more.\n",
    "    )\n",
    "    load_in_8bit = True if torch.cuda.is_available() else False\n",
    "    trust_remote_code = True\n",
    "    if MODEL in [\"mosaicml/mpt-7b\" ]:\n",
    "        torch_data_type =  torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        print(f\"torch_data_type: {torch_data_type}\")\n",
    "        if torch.cuda.is_bf16_supported():\n",
    "            config.attn_config['attn_impl'] = 'triton' # To use the optimized triton implementation of FlashAttention, you can load the model with attn_impl='triton' and move the model to bfloat16:\n",
    "        else:\n",
    "            config.attn_config[\"attn_impl\"] = \"flash\" # To use the optimized FlashAttention implementation, you can load the model with attn_impl='flash' and move the model to float16:\n",
    "        config.update({\"max_seq_len\": 4096}) # Although the model was trained with a sequence length of 2048, ALiBi enables users to increase the maximum sequence length during finetuning and/or inference. For example:\n",
    "        load_in_8bit = False\n",
    "        trust_remote_code = True\n",
    "        device_map = None # ValueError: MPTForCausalLM does not support `device_map='auto'` yet.\n",
    "    elif MODEL in [\"distilgpt2\" ]:\n",
    "        trust_remote_code = True\n",
    "    elif MODEL in [\"databricks/dolly-v2-3b\"]:\n",
    "        torch_data_type =  torch.bfloat16  #  You need to execute a model loaded in half precision on a GPU, the operations are not implemented in half on the CPU.\n",
    "        trust_remote_code = True\n",
    "        device_map = \"auto\"\n",
    "        load_in_8bit = False\n",
    "    elif MODEL in [\"databricks/dolly-v2-12b\" , \"databricks/dolly-v2-7b\"]:\n",
    "        torch_data_type =  torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else   torch.float16 if torch.cuda.is_available() else torch.float32 #  You need to execute a model loaded in half precision on a GPU, the operations are not implemented in half on the CPU.\n",
    "        assert torch_data_type != torch.float16 , \"torch_data_type should not be torch.float16\"\n",
    "        trust_remote_code = True\n",
    "        device_map = \"auto\"\n",
    "        load_in_8bit = True\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path = MODEL ,\n",
    "        config = config ,\n",
    "        load_in_8bit = load_in_8bit ,\n",
    "        trust_remote_code = True ,\n",
    "        device_map = device_map ,\n",
    "        torch_dtype =  torch_data_type,\n",
    "        low_cpu_mem_usage = True, # device map is set to auto then low_cpu_mem_usage is automatically set to True\n",
    "    )\n",
    "    model.eval() # PyTorch Eval mode will disable dropout and batch normalization. This is important to have reproducible results during inference.\n",
    "else:\n",
    "    ValueError(f\"MODEL: {MODEL} is not supported\")\n",
    "end_time_to_download_model = time.time()\n",
    "print(f\"Time to download model: {end_time_to_download_model - start_time_to_download_model}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from transformers import Pipeline, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "INTRO_BLURB = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    ")\n",
    "\n",
    "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
    "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "\n",
    "def get_special_token_id(tokenizer: PreTrainedTokenizer, key: str) -> int:\n",
    "    \"\"\"Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
    "    When training, we configure the tokenizer so that the sequences like \"### Instruction:\" and \"### End\" are\n",
    "    treated specially and converted to a single, new token.  This retrieves the token ID each of these keys map to.\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
    "        key (str): the key to convert to a single token\n",
    "    Raises:\n",
    "        RuntimeError: if more than one ID was generated\n",
    "    Returns:\n",
    "        int: the token ID for the given key\n",
    "    \"\"\"\n",
    "    token_ids = tokenizer.encode(key)\n",
    "    if len(token_ids) > 1:\n",
    "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
    "    return token_ids[0]\n",
    "\n",
    "\n",
    "class InstructionTextGenerationPipeline(Pipeline):\n",
    "    def __init__(\n",
    "        self, *args, do_sample: bool = True, max_new_tokens: int = 256, top_p: float = 0.92, top_k: int = 0, **kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize the pipeline\n",
    "        Args:\n",
    "            do_sample (bool, optional): Whether or not to use sampling. Defaults to True.\n",
    "            max_new_tokens (int, optional): Max new tokens after the prompt to generate. Defaults to 128.\n",
    "            top_p (float, optional): If set to float < 1, only the smallest set of most probable tokens with\n",
    "                probabilities that add up to top_p or higher are kept for generation. Defaults to 0.92.\n",
    "            top_k (int, optional): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "                Defaults to 0.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, do_sample=do_sample, max_new_tokens=max_new_tokens, top_p=top_p, top_k=top_k,\n",
    "                         **kwargs)\n",
    "\n",
    "    def _sanitize_parameters(self,\n",
    "                             return_full_text: bool = None,\n",
    "                             **generate_kwargs):\n",
    "        preprocess_params = {}\n",
    "\n",
    "        # newer versions of the tokenizer configure the response key as a special token.  newer versions still may\n",
    "        # append a newline to yield a single token.  find whatever token is configured for the response key.\n",
    "        tokenizer_response_key = next(\n",
    "            (token for token in self.tokenizer.additional_special_tokens if token.startswith(RESPONSE_KEY)), None\n",
    "        )\n",
    "\n",
    "        response_key_token_id = None\n",
    "        end_key_token_id = None\n",
    "        if tokenizer_response_key:\n",
    "            try:\n",
    "                response_key_token_id = get_special_token_id(self.tokenizer, tokenizer_response_key)\n",
    "                end_key_token_id = get_special_token_id(self.tokenizer, END_KEY)\n",
    "\n",
    "                # Ensure generation stops once it generates \"### End\"\n",
    "                generate_kwargs[\"eos_token_id\"] = end_key_token_id\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        forward_params = generate_kwargs\n",
    "        postprocess_params = {\n",
    "            \"response_key_token_id\": response_key_token_id,\n",
    "            \"end_key_token_id\": end_key_token_id\n",
    "        }\n",
    "\n",
    "        if return_full_text is not None:\n",
    "            postprocess_params[\"return_full_text\"] = return_full_text\n",
    "\n",
    "        return preprocess_params, forward_params, postprocess_params\n",
    "\n",
    "    def preprocess(self, instruction_text, **generate_kwargs):\n",
    "        prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n",
    "        inputs = self.tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs[\"prompt_text\"] = prompt_text\n",
    "        inputs[\"instruction_text\"] = instruction_text\n",
    "        return inputs\n",
    "\n",
    "    def _forward(self, model_inputs, **generate_kwargs):\n",
    "        input_ids = model_inputs[\"input_ids\"]\n",
    "        attention_mask = model_inputs.get(\"attention_mask\", None)\n",
    "\n",
    "        if input_ids.shape[1] == 0:\n",
    "            input_ids = None\n",
    "            attention_mask = None\n",
    "            in_b = 1\n",
    "        else:\n",
    "            in_b = input_ids.shape[0]\n",
    "\n",
    "        generated_sequence = self.model.generate(\n",
    "            input_ids=input_ids.to(self.model.device),\n",
    "            attention_mask=attention_mask.to(self.model.device) if attention_mask is not None else None,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            **generate_kwargs,\n",
    "        )\n",
    "\n",
    "        out_b = generated_sequence.shape[0]\n",
    "        if self.framework == \"pt\":\n",
    "            generated_sequence = generated_sequence.reshape(in_b, out_b // in_b, *generated_sequence.shape[1:])\n",
    "        elif self.framework == \"tf\":\n",
    "            generated_sequence = tf.reshape(generated_sequence, (in_b, out_b // in_b, *generated_sequence.shape[1:]))\n",
    "\n",
    "        instruction_text = model_inputs.pop(\"instruction_text\")\n",
    "        return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"instruction_text\": instruction_text}\n",
    "\n",
    "    def postprocess(self, model_outputs, response_key_token_id, end_key_token_id, return_full_text: bool = False):\n",
    "\n",
    "        generated_sequence = model_outputs[\"generated_sequence\"][0]\n",
    "        instruction_text = model_outputs[\"instruction_text\"]\n",
    "\n",
    "        generated_sequence: List[List[int]] = generated_sequence.numpy().tolist()\n",
    "        records = []\n",
    "        for sequence in generated_sequence:\n",
    "\n",
    "            # The response will be set to this variable if we can identify it.\n",
    "            decoded = None\n",
    "\n",
    "            # If we have token IDs for the response and end, then we can find the tokens and only decode between them.\n",
    "            if response_key_token_id and end_key_token_id:\n",
    "                # Find where \"### Response:\" is first found in the generated tokens.  Considering this is part of the\n",
    "                # prompt, we should definitely find it.  We will return the tokens found after this token.\n",
    "                try:\n",
    "                    response_pos = sequence.index(response_key_token_id)\n",
    "                except ValueError:\n",
    "                    logger.warn(f\"Could not find response key {response_key_token_id} in: {sequence}\")\n",
    "                    response_pos = None\n",
    "\n",
    "                if response_pos:\n",
    "                    # Next find where \"### End\" is located.  The model has been trained to end its responses with this\n",
    "                    # sequence (or actually, the token ID it maps to, since it is a special token).  We may not find\n",
    "                    # this token, as the response could be truncated.  If we don't find it then just return everything\n",
    "                    # to the end.  Note that even though we set eos_token_id, we still see the this token at the end.\n",
    "                    try:\n",
    "                        end_pos = sequence.index(end_key_token_id)\n",
    "                    except ValueError:\n",
    "                        end_pos = None\n",
    "\n",
    "                    decoded = self.tokenizer.decode(sequence[response_pos + 1 : end_pos]).strip()\n",
    "\n",
    "            if not decoded:\n",
    "                # Otherwise we'll decode everything and use a regex to find the response and end.\n",
    "\n",
    "                fully_decoded = self.tokenizer.decode(sequence)\n",
    "\n",
    "                # The response appears after \"### Response:\".  The model has been trained to append \"### End\" at the\n",
    "                # end.\n",
    "                m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", fully_decoded, flags=re.DOTALL)\n",
    "\n",
    "                if m:\n",
    "                    decoded = m.group(1).strip()\n",
    "                else:\n",
    "                    # The model might not generate the \"### End\" sequence before reaching the max tokens.  In this case,\n",
    "                    # return everything after \"### Response:\".\n",
    "                    m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded, flags=re.DOTALL)\n",
    "                    if m:\n",
    "                        decoded = m.group(1).strip()\n",
    "                    else:\n",
    "                        logger.warn(f\"Failed to find response in:\\n{fully_decoded}\")\n",
    "\n",
    "            # If the full text is requested, then append the decoded text to the original instruction.\n",
    "            # This technically isn't the full text, as we format the instruction in the prompt the model has been\n",
    "            # trained on, but to the client it will appear to be the full text.\n",
    "            if return_full_text:\n",
    "                decoded = f\"{instruction_text}\\n{decoded}\"\n",
    "\n",
    "            rec = {\"generated_text\": decoded}\n",
    "\n",
    "            records.append(rec)\n",
    "\n",
    "        return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer, task=\"text-generation\")\n",
    "print(generate_text.task)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Prompt with Hugging Face Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# template for an instrution with no input\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\"],\n",
    "    template=\"{instruction}\")\n",
    "\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n",
    "\n",
    "print(llm_chain.predict(instruction=\"Explain to me the difference between nuclear fission and fusion.\").lstrip())\n",
    "\n",
    "context = \"\"\"George Washington (February 22, 1732[b] – December 14, 1799) was an American military officer, statesman,\n",
    "and Founding Father who served as the first president of the United States from 1789 to 1797.\"\"\"\n",
    "\n",
    "print(llm_context_chain.predict(instruction=\"When was George Washington president?\", context=context).lstrip())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, GenerationConfig\n",
    "generation_config =  GenerationConfig(\n",
    "    max_new_tokens = 256, # The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "    num_beams = 2, # 1 means no beam search instead greedy search\n",
    "    temperature = .3, # Parameters for manipulation of the model output logits\n",
    "    top_p = 0.95, # Parameters for manipulation of the model output logits\n",
    "    do_sample = True # select a random token from the top-k tokens (set to 0 to disable top-k sampling) instead of choosing the one with the highest probability\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Mapping for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(model.hf_device_map)\n",
    "except:\n",
    "    print(\"model.hf_device_map is not available\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Paraphrase the Prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parahrase the prompt with the model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom typing import TypedDict\\nfrom torch import LongTensor\\nclass TokenizerOutput(TypedDict):\\n  input_ids: LongTensor\\n  attention_mask: LongTensor\\npotential_prompts = list()\\nfor meta_prompt in list_of_meta_prompt:\\n    tokenized_prompts: TokenizerOutput = tokenizer( meta_prompt, return_tensors='pt', padding=False, add_special_tokens=False)\\n    outputs = model.generate(\\n        input_ids = tokenized_prompts.input_ids.to(model.device) , # model.device is the device where the model is loaded (cpu or gpu\\n        attention_mask = tokenized_prompts.attention_mask.to(model.device) , # model.device is the device where the model is loaded (cpu or gpu\\n        generation_config = generation_config,\\n        do_sample = generation_config.temperature > 0\\n    )\\n    potential_prompts.append(tokenizer.decode(outputs[0] , skip_special_tokens=True))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from typing import TypedDict\n",
    "from torch import LongTensor\n",
    "class TokenizerOutput(TypedDict):\n",
    "  input_ids: LongTensor\n",
    "  attention_mask: LongTensor\n",
    "potential_prompts = list()\n",
    "for meta_prompt in list_of_meta_prompt:\n",
    "    tokenized_prompts: TokenizerOutput = tokenizer( meta_prompt, return_tensors='pt', padding=False, add_special_tokens=False)\n",
    "    outputs = model.generate(\n",
    "        input_ids = tokenized_prompts.input_ids.to(model.device) , # model.device is the device where the model is loaded (cpu or gpu\n",
    "        attention_mask = tokenized_prompts.attention_mask.to(model.device) , # model.device is the device where the model is loaded (cpu or gpu\n",
    "        generation_config = generation_config,\n",
    "        do_sample = generation_config.temperature > 0\n",
    "    )\n",
    "    potential_prompts.append(tokenizer.decode(outputs[0] , skip_special_tokens=True))\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphase the propmt with the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_prompt = [None] * len(list_of_meta_prompt)\n",
    "start_time = time.time()\n",
    "for index, meta_prompt in enumerate(list_of_meta_prompt):\n",
    "    potential_prompt[index] = llm_chain.predict(instruction= meta_prompt).lstrip()\n",
    "end_time = time.time()\n",
    "print(f\"Time to generate 10 prompts: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'potential_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/null/code/PromptGPT/implement-demystify_prompt.ipynb Cell 29\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/null/code/PromptGPT/implement-demystify_prompt.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(potential_prompt)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/null/code/PromptGPT/implement-demystify_prompt.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#potential_prompt.append(seed_prompt)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/null/code/PromptGPT/implement-demystify_prompt.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m## remove all empty strings\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/null/code/PromptGPT/implement-demystify_prompt.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m potential_prompt \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, potential_prompt))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'potential_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "print(potential_prompt)\n",
    "potential_prompt.append(seed_prompt)\n",
    "## remove all empty strings\n",
    "try:\n",
    "    potential_prompt = list(filter(\"\", potential_prompt))\n",
    "except:\n",
    "    print(\"potential_prompt is not a list of strings\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Semantic Search in using a Sentence Transformers in CPU compare which prompt are good"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BackTranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_german = [None] * len(potential_prompt)\n",
    "pp_temp = len(potential_prompt)\n",
    "potential_prompt+= [None] * pp_temp\n",
    "for index in range(0 , len(potential_prompt)):\n",
    "    eng_german[index] = llm_chain.predict(instruction= f\"translate English to German: {potential_prompt[index]}\").lstrip()\n",
    "    potential_prompt[index + pp_temp] = llm_chain.predict(instruction= f\"translate German to English: {eng_german[index]}\").lstrip()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio Demo: text_generation\n",
    "### This text generation demo takes in input text and returns generated text. It uses the Transformers library to set up the model and has two examples.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def generate(text):\n",
    "    #result = generator(text, max_length=30, num_return_sequences=1)\n",
    "    return llm_chain.predict(instruction = text ).lstrip()\n",
    "\n",
    "examples = [\n",
    "    [\"The Moon's orbit around Earth has\"],\n",
    "    [\"The smooth Borealis basin in the Northern Hemisphere covers 40%\"],\n",
    "]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate,\n",
    "    inputs=gr.inputs.Textbox(lines=5, label=\"Input Text\"),\n",
    "    outputs=gr.outputs.Textbox(label=\"Generated Text\"),\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "device = \"cuda\"\n",
    "model_id = \"databricks/dolly-v2-3b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temoctalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
