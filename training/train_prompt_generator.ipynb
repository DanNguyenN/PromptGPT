{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all the Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the copmuter is on google colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    !pip install rich\n",
    "    # Install PyTorch 2.0 with cuda 11.7\n",
    "    !pip install \"torch>=2.0\" --extra-index-url https://download.pytorch.org/whl/cu117 --upgrade --quiet\n",
    "    !pip install -q -U bitsandbytes\n",
    "    !pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "    !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "    !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "    !pip install datasets\n",
    "    !pip install wandb\n",
    "    #!pip install ray[tune]\n",
    "    !pip install langchain\n",
    "    !pip install session-info\n",
    "    !pip install tensorboard\n",
    "else:\n",
    "    print(\"Not running on Google Colab\")\n",
    "from rich import print\n",
    "import logging\n",
    "from pathlib import Path\n",
    "logger = logging.getLogger(__name__)\n",
    "#ROOT_PATH = Path(__file__).parent.parent\n",
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the GPU env\n",
    "1. You can check the GPU in the Google Colab by clicking  and efficieny\n",
    "2. Check if the GPU can use bfloat16 most effective as most model are pre-trained with bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from rich import print\n",
    "if torch.cuda.is_available():\n",
    "    !nvidia-smi\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"Cuda capability: \", torch.cuda.get_device_capability(0))\n",
    "    '''\n",
    "    On pre-ampere hardware bf16 works, but doesn't provide speed-ups compared to fp32 matmul operations, and some matmul operations are failing outright, so this check is more like \"guaranteed to work and be performant\" than \"works somehow\".  https://github.com/pytorch/pytorch/issues/75427\n",
    "    '''\n",
    "    print(f\"bfloat16 support: { torch.cuda.is_bf16_supported()}\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Seed Environment of the Notebook to ensure the reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "set_seed( DEFAULT_SEED )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Default title text\n",
    "variable_name = \"paged_lion_8bit\" #@param [\"paged_lion_8bit\"]\n",
    "BITS  = 4\n",
    "OPTIMIZER = \"paged_lion_8bit\"\n",
    "# Whether to use bf16 (preferred on A100's).\"\n",
    "# Default now is in pyTorch 1.13 and above\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Weight And Bias ðŸ’¡ Configuration tips\n",
    "\n",
    "W&B integration with Hugging Face can be configured to add extra functionalities:\n",
    "\n",
    "* auto-logging of models as artifacts: just set environment varilable `WANDB_LOG_MODEL` to `true`\n",
    "* log histograms of gradients and parameters: by default gradients are logged, you can also log parameters by setting environment variable `WANDB_WATCH` to `all`\n",
    "* set custom run names with `run_name` arg present in scripts or as part of `TrainingArguments`\n",
    "* organize runs by project with the `WANDB_PROJECT` environment variable\n",
    "\n",
    "For more details refer to [W&B + HF integration documentation](https://docs.wandb.ai/integrations/huggingface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "%env WANDB_LOG_MODEL=true\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"prompt_generator\"\n",
    "\n",
    "# save your trained model checkpoint to wandb you don't want to save it set this to false as the model is 3 billion saize \n",
    "#os.environ[\"WANDB_LOG_MODEL\"]\n",
    "\n",
    "# turn off watch to log faster\n",
    "#os.environ[\"WANDB_WATCH\"]= \"all\" # I am getting an error with this\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Local Training Root"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Datset from the Hugging Face Datset Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TRAINING_DATASET = \"Rami/prompts\"\n",
    "from datasets import load_dataset\n",
    "training_dataset = load_dataset(\n",
    "    \"Rami/prompts\",\n",
    ")\n",
    "print(\n",
    "    training_dataset\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Tokenizers\n",
    "1. We are suing Dolly model which was trained on the Pythia model. Instead we are recreating the dollvy tokenizer from the Pythia tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXTokenizerFast(name_or_path='databricks/dolly-v2-3b', vocab_size=50254, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['### End', '### Instruction:', '### Response:']}, clean_up_tokenization_spaces=True)\n",
      "GPTNeoXTokenizerFast(name_or_path='EleutherAI/pythia-6.9b', vocab_size=50254, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)\n",
      "GPTNeoXTokenizerFast(name_or_path='EleutherAI/pythia-6.9b', vocab_size=50254, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['### End', '### Instruction:', '### Response:']}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Special Tokens\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "INPUT_KEY = \"Input:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "RESPONSE_KEY_NL = f\"{RESPONSE_KEY}\\n\"\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"databricks/dolly-v2-3b\"#\"databricks/dolly-v2-3b\"\n",
    "eleutherai_python_3b = \"EleutherAI/pythia-2.8b\"\n",
    "eleutherai_python_7b = \"EleutherAI/pythia-6.9b\"\n",
    "dolly_v2_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "print(dolly_v2_tokenizer)\n",
    "pythia_tokenizer = AutoTokenizer.from_pretrained(eleutherai_python_7b ,\n",
    "                                                 padding_side  = \"right\")\n",
    "                                                 \n",
    "print(pythia_tokenizer)\n",
    "\n",
    "# Make sure that the pad token is the end of the tokens\n",
    "pythia_tokenizer.pad_token = pythia_tokenizer.eos_token\n",
    "\n",
    "# Add special tokens for End , Instruction , Response Key\n",
    "pythia_tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\n",
    "        END_KEY,\n",
    "        INSTRUCTION_KEY,\n",
    "        RESPONSE_KEY,\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(pythia_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the dataset\n",
    "1. Convert the dataset into instruction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruct Format String and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTRO_BLURB = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    ")\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "INPUT_KEY = \"Input:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "RESPONSE_KEY_NL = f\"{RESPONSE_KEY}\\n\"\n",
    "DEFAULT_SEED = 42\n",
    "# This is a training prompt that does not contain an input string.  The instruction by itself has enough information\n",
    "# to respond.  For example, the instruction might ask for the year a historic figure was born.\n",
    "PROMPT_NO_INPUT_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{response_key}\n",
    "{response}\n",
    "\n",
    "{end_key}\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    "    response=\"{response}\",\n",
    "    end_key=END_KEY,\n",
    ")\n",
    "\n",
    "# This is a training prompt that contains an input string that serves as context for the instruction.  For example,\n",
    "# the input might be a passage from Wikipedia and the intruction is to extract some information from it.\n",
    "PROMPT_WITH_INPUT_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{input_key}\n",
    "{input}\n",
    "\n",
    "{response_key}\n",
    "{response}\n",
    "\n",
    "{end_key}\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    input_key=INPUT_KEY,\n",
    "    input=\"{input}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    "    response=\"{response}\",\n",
    "    end_key=END_KEY,\n",
    ")\n",
    "\n",
    "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
    "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "def load_training_dataset(path_or_dataset: str = DEFAULT_TRAINING_DATASET) -> Dataset:\n",
    "    logger.info(f\"Loading dataset from {path_or_dataset}\")\n",
    "    dataset = load_dataset(path_or_dataset)[\"train\"]\n",
    "    logger.info(\"Found %d rows\", dataset.num_rows)\n",
    "\n",
    "    def _add_text(rec):\n",
    "        instruction = rec[\"instruction\"]\n",
    "        response = rec[\"response\"]\n",
    "        context = rec.get(\"context\")\n",
    "\n",
    "        if not instruction:\n",
    "            raise ValueError(f\"Expected an instruction in: {rec}\")\n",
    "\n",
    "        if not response:\n",
    "            raise ValueError(f\"Expected a response in: {rec}\")\n",
    "\n",
    "        # For some instructions there is an input that goes along with the instruction, providing context for the\n",
    "        # instruction.  For example, the input might be a passage from Wikipedia and the instruction says to extract\n",
    "        # some piece of information from it.  The response is that information to extract.  In other cases there is\n",
    "        # no input.  For example, the instruction might be open QA such as asking what year some historic figure was\n",
    "        # born.\n",
    "        if context:\n",
    "            rec[\"text\"] = PROMPT_WITH_INPUT_FORMAT.format(instruction=instruction, response=response, input=context)\n",
    "        else:\n",
    "            rec[\"text\"] = PROMPT_NO_INPUT_FORMAT.format(instruction=instruction, response=response)\n",
    "        return rec\n",
    "\n",
    "    dataset = dataset.map(_add_text)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the dataset methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    PreTrainedTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "def preprocess_batch(batch: Dict[str, List], tokenizer: AutoTokenizer, max_length: int) -> dict:\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation = True,\n",
    "    )\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed=DEFAULT_SEED, training_dataset: str = DEFAULT_TRAINING_DATASET) -> Dataset:\n",
    "    \"\"\"Loads the training dataset and tokenizes it so it is ready for training.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (AutoTokenizer): Tokenizer tied to the model.\n",
    "        max_length (int): Maximum number of tokens to emit from tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: HuggingFace dataset\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = load_training_dataset(training_dataset)\n",
    "\n",
    "    logger.info(\"Preprocessing dataset\")\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "    )\n",
    "\n",
    "    # Make sure we don't have any truncated records, as this would mean the end keyword is missing.\n",
    "    logger.info(\"Processed dataset has %d rows\", dataset.num_rows)\n",
    "    dataset = dataset.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)\n",
    "    logger.info(\"Processed dataset has %d rows after filtering for truncated records\", dataset.num_rows)\n",
    "\n",
    "    logger.info(\"Shuffling dataset\")\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    logger.info(\"Done preprocessing\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator For Completion Only LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# The author use the Data Collator for Adding special tokens, such as the beginning-of-sequence and end-of-sequence tokens.\n",
    "class DataCollatorForCompletionOnlyLM(DataCollatorForLanguageModeling):\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        # The prompt ends with the response key plus a newline.  We encode this and then try to find it in the\n",
    "        # sequence of tokens.  This should just be a single token.\n",
    "        response_token_ids = self.tokenizer.encode(RESPONSE_KEY_NL)\n",
    "\n",
    "        labels = batch[\"labels\"].clone()\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "\n",
    "            response_token_ids_start_idx = None\n",
    "            for idx in np.where(batch[\"labels\"][i] == response_token_ids[0])[0]:\n",
    "                response_token_ids_start_idx = idx\n",
    "                break\n",
    "\n",
    "            if response_token_ids_start_idx is None:\n",
    "                raise RuntimeError(\n",
    "                    f'Could not find response key {response_token_ids} in token IDs {batch[\"labels\"][i]}'\n",
    "                )\n",
    "\n",
    "            response_token_ids_end_idx = response_token_ids_start_idx + 1\n",
    "\n",
    "            # Make pytorch loss function ignore all tokens up through the end of the response key\n",
    "            labels[i, :response_token_ids_end_idx] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Model\n",
    "1. Torch Datat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Bits and Butes Config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Bit Configuration\n",
    "1. 4 bit Normal Float \n",
    "2. Double Quantization save even more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neox-20b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit = False,\n",
    "    bnb_4bit_use_double_quant=True , # double quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the LM Models\n",
    "Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "assert torch.cuda.is_available(), \"You need to have a GPU to run this notebook.\"\n",
    "n_gpus = torch.cuda.device_count()\n",
    "def model_init():\n",
    "    free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "    max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {n_gpus}\")\n",
    "    max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "    print(f\"Max memory: {max_memory}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path = PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "        trust_remote_code = True,\n",
    "        use_cache = False,\n",
    "        torch_dtype =  torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        device_map = \"auto\",\n",
    "        load_in_8bit = False,\n",
    "        load_in_4bit = True,\n",
    "        low_cpu_mem_usage = True, # low cpu memory usage is to be true when the device map is auto\n",
    "        max_memory =  max_memory,\n",
    "        quantization_config = bnb_config,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = model_init()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the Max Length of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same max length that the model supports.  Fall back to 1024 if the setting can't be found.\n",
    "# The configuraton for the length can be stored under different names depending on the model.  Here we attempt\n",
    "# a few possible names we've encountered.\n",
    "conf = model.config\n",
    "max_length = None\n",
    "for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "    max_length = getattr(model.config, length_setting, None)\n",
    "    if max_length:\n",
    "        logger.info(f\"Found max lenth: {max_length}\")\n",
    "        break\n",
    "if not max_length:\n",
    "    max_length = 1024\n",
    "    logger.info(f\"Using default max length: {max_length}\")\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing#scrollTo=jq0nX33BmfaC\n",
    "from peft import prepare_model_for_kbit_training\n",
    "def print_trainable_parameters( model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    if BITS == 4: trainable_params /= 2\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || \"\n",
    "        f\"all params: {all_param} || \"\n",
    "        f\"trainable: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "# https://github.com/huggingface/peft/blob/main/src/peft/utils/other.py#LL62C1-L97C17\n",
    "model = prepare_model_for_kbit_training(model , use_gradient_checkpointing = True)\n",
    "# in the Qlora codes they did gradient checkpointing twice\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Q LoRa Models\n",
    "1. The Q Lora add adapters at every network layers and therby avoid almost all of the accuracy tradeoff seen in priors works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "import bitsandbytes as bnb\n",
    "def find_all_linear_names( model):\n",
    "    cls = bnb.nn.Linear4bit if BITS == 4 else (bnb.nn.Linear8bitLt if BITS == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "print(find_all_linear_names(model)) # ['query_key_value', 'dense_h_to_4h', 'dense_4h_to_h', 'dense'] adpater is added to the layer in QLoRa papers\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # there iosnot relation between R and performance of the model \n",
    "    lora_alpha = 16 , # Q Lora they set alpha to 16\n",
    "    target_modules = find_all_linear_names(model), \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(model)\n",
    "# https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "for name, module in model.named_modules():\n",
    "    #if isinstance(module, LoraLayer):\n",
    "        #if args.bf16:\n",
    "        #    module = module.to(torch.bfloat16)\n",
    "    if 'norm' in name:\n",
    "        module = module.to(torch.float32)\n",
    "    #if 'lm_head' in name or 'embed_tokens' in name:\n",
    "    #    if hasattr(module, 'weight'):\n",
    "    #        if args.bf16 and module.weight.dtype == torch.float32:\n",
    "    #            module = module.to(torch.bfloat16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists, join, isdir\n",
    "import transformers\n",
    "PREFIX_CHECKPOINT_DIR = None\n",
    "class SavePeftModelCallback(transformers.TrainerCallback):\n",
    "    def save_model(self, args, state, kwargs):\n",
    "        print('Saving PEFT checkpoint...')\n",
    "        if state.best_model_checkpoint is not None:\n",
    "            checkpoint_folder = os.path.join(state.best_model_checkpoint, \"adapter_model\")\n",
    "        else:\n",
    "            checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        self.save_model(args, state, kwargs)\n",
    "        return control\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        def touch(fname, times=None):\n",
    "            with open(fname, 'a'):\n",
    "                os.utime(fname, times)\n",
    "\n",
    "        touch(join(args.output_dir, 'completed'))\n",
    "        self.save_model(args, state, kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Datset using the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = preprocess_dataset(\n",
    "    tokenizer = pythia_tokenizer,\n",
    "    max_length = max_length,\n",
    "    seed = DEFAULT_SEED,\n",
    ")\n",
    "\n",
    "# Split the Dataset\n",
    "\n",
    "split_dataset = processed_dataset.train_test_split(test_size = 0.2, seed=DEFAULT_SEED, shuffle = True)\n",
    "\n",
    "train_datset = split_dataset[\"train\"]\n",
    "print(train_datset)\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "logger.info(\"Train data size: %d\", split_dataset[\"train\"].num_rows)\n",
    "logger.info(\"Test data size: %d\", split_dataset[\"test\"].num_rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Data Collator for this Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer = pythia_tokenizer,\n",
    "    mlm = False , \n",
    "    return_tensors=\"pt\", \n",
    "    pad_to_multiple_of = 8\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Arguments \n",
    "1. WarmUp Steps \n",
    "2. Learning Rate \n",
    "   1.  0.0001 <path_or_name> https://github.com/artidoro/qlora/tree/main if the model is bigger than 13 Billions\n",
    "   2.  The learning rate of the lora for gpt model was 2.00e-04. I belive the learning rate need to be higher for the lora model\n",
    "   3.  What should be the learnign rate of the model 32 batch size dollvy model\n",
    "       1.  2.00e-04 .3 \n",
    "       2.  2.00e-03\n",
    "       3.  1e-5\n",
    "       4.  0.001\n",
    "       5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_run_name():\n",
    "    import random\n",
    "    return f\"run-{PRETRAINED_MODEL_NAME_OR_PATH.replace('/', '-')}-{BITS}bit-LoRa-paged-adamw-8bits\"+(\"-bf16\" if torch.cuda.is_bf16_supported() else \"\") + str(random.randint(0,9))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The larger the batch size the worse of the performance of the model, and harder to overfit and can easily get diverged.\n",
    "'''\n",
    "import torch\n",
    "def determine_batch_size(optimizer:str = \"adamw\", bits:int = 4, gpus:int = 1):\n",
    "    if optimizer == \"paded_adamw_8bit\" and bits == 4 and \"A100\" in torch.cuda.get_device_name(0):\n",
    "        return 32\n",
    "    elif optimizer == \"paded_lion_8bit\" and bits == 4 and \"A100\" in torch.cuda.get_device_name(0):\n",
    "        return 64\n",
    "    elif optimizer == \"paded_adamw_8bit\" and bits == 4 and \"A100\" not in torch.cuda.get_device_name(0):\n",
    "        return 8\n",
    "    elif optimizer ==\"paged_lion_8bit\" and bits == 4 and \"A100\" not in torch.cuda.get_device_name(0):\n",
    "        return 4\n",
    "    else:\n",
    "        # Determine the Batch Size for 4 bits and non A100 GPU\n",
    "        return 2\n",
    "print(determine_batch_size(OPTIMIZER, BITS, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments , EarlyStoppingCallback\n",
    "from packaging import version\n",
    "batch_size: int = determine_batch_size(optimizer = OPTIMIZER, bits = BITS, gpus = n_gpus)\n",
    "gradient_accumulation_steps = 4\n",
    "print(f\"My effictive batch size is {batch_size * gradient_accumulation_steps}\")\n",
    "learning_rate: float = 2e-4\n",
    "epochs = 16 if \"NVIDIA A100-SXM4-40GB\" in torch.cuda.get_device_name(0) else 4\n",
    "print(f\"learning_rate: {learning_rate}\")\n",
    "trainer_arguments = TrainingArguments(\n",
    "            output_dir = \"outputs\",\n",
    "            per_device_train_batch_size = batch_size,\n",
    "            per_device_eval_batch_size = batch_size,\n",
    "            gradient_accumulation_steps = gradient_accumulation_steps, # https://lightning.ai/pages/blog/gradient-accumulation/\n",
    "            num_train_epochs = epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            lr_scheduler_type = \"constant\", # q lora paper they used constant learning rate and consine \n",
    "            warmup_ratio = .8, # notice the model stop decreae after it reach the peas\n",
    "            bf16 =  True if torch.cuda.is_bf16_supported() else False,\n",
    "            fp16 =   False if  torch.cuda.is_bf16_supported() else True,\n",
    "            bf16_full_eval = True if torch.cuda.is_bf16_supported() else False,\n",
    "            logging_steps = 1, \n",
    "            eval_steps = 1,\n",
    "            evaluation_strategy = \"steps\",\n",
    "            optim = OPTIMIZER,\n",
    "            torch_compile =  True if version.parse(torch.__version__) >= version.parse(\"2.0.0\") and BITS > 8 else False,\n",
    "            save_strategy=\"steps\",\n",
    "            report_to = [\"wandb\", \"tensorboard\"],\n",
    "            load_best_model_at_end = True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            seed = DEFAULT_SEED,\n",
    "            max_grad_norm = .3 # the max grad norm was set to 0.3 in the Q Lora Papres\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer = pythia_tokenizer,\n",
    "    args = trainer_arguments,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [ EarlyStoppingCallback(\n",
    "        early_stopping_patience = 2 if \"NVIDIA A100-SXM4-40GB\" in torch.cuda.get_device_name(0) else 1 , \n",
    "        early_stopping_threshold = 0.005\n",
    "        )\n",
    "                 ],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logger.info(\"Training\")\n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "except RuntimeError as e:\n",
    "    if 'out of memory' in str(e):\n",
    "        print('| WARNING: ran out of memory, retrying batch')\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                del p.grad  # free some memory\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = trainer.state.log_history\n",
    "print(train_logs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extensio\n",
    "# https://discuss.huggingface.co/t/how-to-read-the-logs-created-by-hugging-face-trainer/32279/4\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push the Model to the Hugging Face Model Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"Rami/dolly_prompt_generator\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation COnfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, GenerationConfig\n",
    "import random\n",
    "generation_config =  GenerationConfig(\n",
    "    max_new_tokens = 256, # The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "    num_beams = 1, # 1 means no beam search instead greedy search\n",
    "    temperature = .3, # Parameters for manipulation of the model output logits\n",
    "    top_p = 0.92, # Parameters for manipulation of the model output logits\n",
    "    top_k = 50, # Parameters to only select the top-k tokens, instead of sampling from the distribution\n",
    "    do_sample = True ,# select a random token from the top-k tokens (set to 0 to disable top-k sampling) instead of choosing the one with the highest probability\n",
    "    use_cache = True, # Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
    "    repetition_penalty = 1.02, # The parameter for repetition penalty. 1.0 means no penalty. See this paper for more details.\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the adaper config if not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "repo_name = \"Rami/dolly_prompt_generator\"\n",
    "config = PeftConfig.from_pretrained(repo_name) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Model and Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "inference_model = None\n",
    "try:\n",
    "    inference_model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        repo_name,\n",
    "    )\n",
    "except NameError as e:\n",
    "    ## Donwload the model from the HFhub\n",
    "    model = model_init()\n",
    "    \n",
    "    inference_model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        repo_name,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Instruction Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from transformers import Pipeline, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "INTRO_BLURB = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    ")\n",
    "\n",
    "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
    "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "\n",
    "def get_special_token_id(tokenizer: PreTrainedTokenizer, key: str) -> int:\n",
    "    \"\"\"Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
    "    When training, we configure the tokenizer so that the sequences like \"### Instruction:\" and \"### End\" are\n",
    "    treated specially and converted to a single, new token.  This retrieves the token ID each of these keys map to.\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
    "        key (str): the key to convert to a single token\n",
    "    Raises:\n",
    "        RuntimeError: if more than one ID was generated\n",
    "    Returns:\n",
    "        int: the token ID for the given key\n",
    "    \"\"\"\n",
    "    token_ids = tokenizer.encode(key)\n",
    "    if len(token_ids) > 1:\n",
    "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
    "    return token_ids[0]\n",
    "\n",
    "from transformers import AutoModelForCausalLM, GenerationConfig\n",
    "class InstructionTextGenerationPipeline(Pipeline):\n",
    "    def __init__(\n",
    "        self, \n",
    "        generation_config: GenerationConfig = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Initialize the pipeline\n",
    "        Args:\n",
    "            do_sample (bool, optional): Whether or not to use sampling. Defaults to True.\n",
    "            max_new_tokens (int, optional): Max new tokens after the prompt to generate. Defaults to 128.\n",
    "            top_p (float, optional): If set to float < 1, only the smallest set of most probable tokens with\n",
    "                probabilities that add up to top_p or higher are kept for generation. Defaults to 0.92.\n",
    "            top_k (int, optional): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "                Defaults to 0.\n",
    "        \"\"\"\n",
    "        self.generation_config: GenerationConfig = generation_config\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _sanitize_parameters(self,\n",
    "                             return_full_text: bool = None,\n",
    "                             **generate_kwargs):\n",
    "        preprocess_params = {}\n",
    "        assert self.generation_config is not None, \"Generation config is not initialized.\"\n",
    "\n",
    "        # newer versions of the tokenizer configure the response key as a special token.  newer versions still may\n",
    "        # append a newline to yield a single token.  find whatever token is configured for the response key.\n",
    "        tokenizer_response_key = next(\n",
    "            (token for token in self.tokenizer.additional_special_tokens if token.startswith(RESPONSE_KEY)), None\n",
    "        )\n",
    "\n",
    "        response_key_token_id = None\n",
    "        end_key_token_id = None\n",
    "        if tokenizer_response_key:\n",
    "            try:\n",
    "                response_key_token_id = get_special_token_id(self.tokenizer, tokenizer_response_key)\n",
    "                end_key_token_id = get_special_token_id(self.tokenizer, END_KEY)\n",
    "\n",
    "                # Ensure generation stops once it generates \"### End\"\n",
    "                generate_kwargs[\"eos_token_id\"] = end_key_token_id\n",
    "                self.generation_config.eos_token_id = end_key_token_id\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        forward_params = generate_kwargs\n",
    "        postprocess_params = {\n",
    "            \"response_key_token_id\": response_key_token_id,\n",
    "            \"end_key_token_id\": end_key_token_id\n",
    "        }\n",
    "\n",
    "        if return_full_text is not None:\n",
    "            postprocess_params[\"return_full_text\"] = return_full_text\n",
    "            print(postprocess_params)\n",
    "\n",
    "        return preprocess_params, forward_params, postprocess_params\n",
    "\n",
    "    def preprocess(self, instruction_text, **generate_kwargs):\n",
    "        prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n",
    "        inputs = self.tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs[\"prompt_text\"] = prompt_text\n",
    "        inputs[\"instruction_text\"] = instruction_text\n",
    "        return inputs\n",
    "    ## Only Once\n",
    "    def _forward(self, model_inputs , eos_token_id):\n",
    "        assert self.model is not None, \"Model is not initialized.\"\n",
    "        assert self.generation_config is not None, \"Generation config is not initialized.\"\n",
    "        assert self.tokenizer is not None, \"Tokenizer is not initialized.\"\n",
    "        assert self.tokenizer.pad_token_id is not None, \"Tokenizer does not have a pad token ID.\"\n",
    "        print(self.generation_config)\n",
    "        input_ids = model_inputs[\"input_ids\"]\n",
    "        attention_mask = model_inputs.get(\"attention_mask\", None)\n",
    "\n",
    "        if input_ids.shape[1] == 0:\n",
    "            input_ids = None\n",
    "            attention_mask = None\n",
    "            in_b = 1\n",
    "        else:\n",
    "            in_b = input_ids.shape[0]\n",
    "\n",
    "        generated_sequence = self.model.generate(\n",
    "            input_ids=input_ids.to(self.model.device),\n",
    "            attention_mask=attention_mask.to(self.model.device) if attention_mask is not None else None,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            generation_config=self.generation_config,\n",
    "        )\n",
    "\n",
    "        out_b = generated_sequence.shape[0]\n",
    "        if self.framework == \"pt\":\n",
    "            generated_sequence = generated_sequence.reshape(in_b, out_b // in_b, *generated_sequence.shape[1:])\n",
    "        elif self.framework == \"tf\":\n",
    "            generated_sequence = tf.reshape(generated_sequence, (in_b, out_b // in_b, *generated_sequence.shape[1:]))\n",
    "\n",
    "        instruction_text = model_inputs.pop(\"instruction_text\")\n",
    "        return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"instruction_text\": instruction_text}\n",
    "\n",
    "    def postprocess(self, model_outputs, response_key_token_id, end_key_token_id, return_full_text: bool = False):\n",
    "\n",
    "        generated_sequence = model_outputs[\"generated_sequence\"][0]\n",
    "        instruction_text = model_outputs[\"instruction_text\"]\n",
    "\n",
    "        generated_sequence: List[List[int]] = generated_sequence.numpy().tolist()\n",
    "        records = []\n",
    "        for sequence in generated_sequence:\n",
    "\n",
    "            # The response will be set to this variable if we can identify it.\n",
    "            decoded = None\n",
    "\n",
    "            # If we have token IDs for the response and end, then we can find the tokens and only decode between them.\n",
    "            if response_key_token_id and end_key_token_id:\n",
    "                # Find where \"### Response:\" is first found in the generated tokens.  Considering this is part of the\n",
    "                # prompt, we should definitely find it.  We will return the tokens found after this token.\n",
    "                try:\n",
    "                    response_pos = sequence.index(response_key_token_id)\n",
    "                except ValueError:\n",
    "                    logger.warn(f\"Could not find response key {response_key_token_id} in: {sequence}\")\n",
    "                    response_pos = None\n",
    "\n",
    "                if response_pos:\n",
    "                    # Next find where \"### End\" is located.  The model has been trained to end its responses with this\n",
    "                    # sequence (or actually, the token ID it maps to, since it is a special token).  We may not find\n",
    "                    # this token, as the response could be truncated.  If we don't find it then just return everything\n",
    "                    # to the end.  Note that even though we set eos_token_id, we still see the this token at the end.\n",
    "                    try:\n",
    "                        end_pos = sequence.index(end_key_token_id)\n",
    "                    except ValueError:\n",
    "                        end_pos = None\n",
    "\n",
    "                    decoded = self.tokenizer.decode(sequence[response_pos + 1 : end_pos]).strip()\n",
    "\n",
    "            if not decoded:\n",
    "                # Otherwise we'll decode everything and use a regex to find the response and end.\n",
    "\n",
    "                fully_decoded = self.tokenizer.decode(sequence)\n",
    "\n",
    "                # The response appears after \"### Response:\".  The model has been trained to append \"### End\" at the\n",
    "                # end.\n",
    "                m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", fully_decoded, flags=re.DOTALL)\n",
    "\n",
    "                if m:\n",
    "                    decoded = m.group(1).strip()\n",
    "                else:\n",
    "                    # The model might not generate the \"### End\" sequence before reaching the max tokens.  In this case,\n",
    "                    # return everything after \"### Response:\".\n",
    "                    m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded, flags=re.DOTALL)\n",
    "                    if m:\n",
    "                        decoded = m.group(1).strip()\n",
    "                    else:\n",
    "                        logger.warn(f\"Failed to find response in:\\n{fully_decoded}\")\n",
    "\n",
    "            # If the full text is requested, then append the decoded text to the original instruction.\n",
    "            # This technically isn't the full text, as we format the instruction in the prompt the model has been\n",
    "            # trained on, but to the client it will appear to be the full text.\n",
    "            if return_full_text:\n",
    "                decoded = f\"{instruction_text}\\n{decoded}\"\n",
    "\n",
    "            rec = {\"generated_text\": decoded}\n",
    "\n",
    "            records.append(rec)\n",
    "\n",
    "        return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "generate_text = InstructionTextGenerationPipeline(model=model,\n",
    "                                                  tokenizer = pythia_tokenizer, \n",
    "                                                  task=\"text-generation\" , \n",
    "                                                  return_full_text=True,\n",
    "                                                  generation_config=generation_config)\n",
    "                                                  \n",
    "print(generate_text.task)\n",
    "print(generate_text._sanitize_parameters())\n",
    "print(generate_text(\"### Instruction: What is the capital of France? ### Response:\"))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Prompt with Hugging Face Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# template for an instrution with no input\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\"],\n",
    "    template=\"{instruction}\")\n",
    "\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n",
    "\n",
    "\n",
    "\n",
    "print(llm_chain.predict(instruction=\"Explain to me the difference between nuclear fission and fusion.\").lstrip())\n",
    "\n",
    "context = \"\"\"George Washington (February 22, 1732[b] â€“ December 14, 1799) was an American military officer, statesman,\n",
    "and Founding Father who served as the first president of the United States from 1789 to 1797.\"\"\"\n",
    "\n",
    "print(llm_context_chain.predict(instruction=\"When was George Washington president?\", context=context).lstrip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_int():\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] [Dolly Github](https://github.com/databrickslabs/dolly/blob/5021d941d95dddcf1f00d978d7f944709873f419/training/trainer.py#L138)\n",
    "[2] https://gist.github.com/Birch-san/57878c4a27cf34f57d3e861865a7d0a2\n",
    "[3] https://github.com/artidoro/qlora/blob/main/qlora.py \n",
    "[4] https://github.com/tloen/alpaca-lora/blob/main/finetune.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temoctalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
