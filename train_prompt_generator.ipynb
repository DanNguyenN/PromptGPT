{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all the Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Google Colab\n"
     ]
    }
   ],
   "source": [
    "# Check if the copmuter is on google colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    !pip install rich\n",
    "    !pip install \"accelerate>=0.16.0,<1\" \n",
    "    !pip install \"torch>=1.13.1\"\n",
    "    !pip install \"transformers[torch]>=4.28.1,<5\" \n",
    "    !pip install \"datasets>=1.14.0,<2\"\n",
    "    !pip install bitsandbytes\n",
    "    !pip install sentencepiece\n",
    "    !pip install triton\n",
    "    !pip install einops\n",
    "    !pip install safetensors\n",
    "    !pip install langchain\n",
    "    !pip install gradio\n",
    "    !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "    !pip install -q datasets\n",
    "else:\n",
    "    print(\"Not running on Google Colab\")\n",
    "from rich import print\n",
    "import logging\n",
    "from pathlib import Path\n",
    "logger = logging.getLogger(__name__)\n",
    "ROOT_PATH = Path(__file__).parent.parent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the GPU env\n",
    "1. You can check the GPU in the Google Colab by clicking  and efficieny\n",
    "2. Check if the GPU can use bfloat16 most effective as most model are pre-trained with bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from rich import print\n",
    "if torch.cuda.is_available():\n",
    "    !nvidia-smi\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"Cuda capability: \", torch.cuda.get_device_capability(0))\n",
    "    '''\n",
    "    On pre-ampere hardware bf16 works, but doesn't provide speed-ups compared to fp32 matmul operations, and some matmul operations are failing outright, so this check is more like \"guaranteed to work and be performant\" than \"works somehow\".  https://github.com/pytorch/pytorch/issues/75427\n",
    "    '''\n",
    "    print(f\"bfloat16 support: { torch.cuda.is_bf16_supported()}\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Seed Environment of the Notebook to ensure the reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "set_seed( DEFAULT_SEED )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Datset from the Hugging Face Datset Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/null/.cache/huggingface/datasets/Rami___parquet/Rami--prompts-a4a4f069a7addc53/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654560e1223f4401bfef0235d6780747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetDict</span><span style=\"font-weight: bold\">({</span>\n",
       "    train: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'base_prompt'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'improved_prompt'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'views'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">341</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "<span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDatasetDict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "    train: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'base_prompt'\u001b[0m, \u001b[32m'improved_prompt'\u001b[0m, \u001b[32m'views'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m341\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\n",
    "    \"Rami/prompts\",\n",
    ")\n",
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Tokenizers\n",
    "1. We are suing Dolly model which was trained on the Pythia model. Instead we are recreating the dollvy tokenizer from the Pythia tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTNeoXTokenizerFast</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name_or_path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'databricks/dolly-v2-3b'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50254</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">model_max_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000000000000000019884624838656</span>, <span style=\"color: #808000; text-decoration-color: #808000\">is_fast</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">padding_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">truncation_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">special_tokens</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'bos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'eos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'unk_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'pad_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'additional_special_tokens'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'### End'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'### Instruction:'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'### Response:'</span><span style=\"font-weight: bold\">]}</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">clean_up_tokenization_spaces</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mGPTNeoXTokenizerFast\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname_or_path\u001b[0m=\u001b[32m'databricks/dolly-v2-3b'\u001b[0m, \u001b[33mvocab_size\u001b[0m=\u001b[1;36m50254\u001b[0m, \n",
       "\u001b[33mmodel_max_length\u001b[0m=\u001b[1;36m1000000000000000019884624838656\u001b[0m, \u001b[33mis_fast\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mpadding_side\u001b[0m=\u001b[32m'right'\u001b[0m, \u001b[33mtruncation_side\u001b[0m=\u001b[32m'right'\u001b[0m, \n",
       "\u001b[33mspecial_tokens\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'bos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'eos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'unk_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \n",
       "\u001b[32m'pad_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'additional_special_tokens'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'### End'\u001b[0m, \u001b[32m'### Instruction:'\u001b[0m, \u001b[32m'### Response:'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m, \n",
       "\u001b[33mclean_up_tokenization_spaces\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTNeoXTokenizerFast</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name_or_path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'EleutherAI/pythia-2.8b'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50254</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">model_max_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000000000000000019884624838656</span>, <span style=\"color: #808000; text-decoration-color: #808000\">is_fast</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">padding_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">truncation_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">special_tokens</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'bos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'eos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'unk_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span><span style=\"font-weight: bold\">}</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">clean_up_tokenization_spaces</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mGPTNeoXTokenizerFast\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname_or_path\u001b[0m=\u001b[32m'EleutherAI/pythia-2.8b'\u001b[0m, \u001b[33mvocab_size\u001b[0m=\u001b[1;36m50254\u001b[0m, \n",
       "\u001b[33mmodel_max_length\u001b[0m=\u001b[1;36m1000000000000000019884624838656\u001b[0m, \u001b[33mis_fast\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mpadding_side\u001b[0m=\u001b[32m'right'\u001b[0m, \u001b[33mtruncation_side\u001b[0m=\u001b[32m'right'\u001b[0m, \n",
       "\u001b[33mspecial_tokens\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'bos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'eos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'unk_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m\u001b[1m}\u001b[0m, \n",
       "\u001b[33mclean_up_tokenization_spaces\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTNeoXTokenizerFast</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name_or_path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'EleutherAI/pythia-2.8b'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50254</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">model_max_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000000000000000019884624838656</span>, <span style=\"color: #808000; text-decoration-color: #808000\">is_fast</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">padding_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">truncation_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">special_tokens</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'bos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'eos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'unk_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'pad_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'additional_special_tokens'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'### End'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'### Instruction:'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'### Response:'</span><span style=\"font-weight: bold\">]}</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">clean_up_tokenization_spaces</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mGPTNeoXTokenizerFast\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname_or_path\u001b[0m=\u001b[32m'EleutherAI/pythia-2.8b'\u001b[0m, \u001b[33mvocab_size\u001b[0m=\u001b[1;36m50254\u001b[0m, \n",
       "\u001b[33mmodel_max_length\u001b[0m=\u001b[1;36m1000000000000000019884624838656\u001b[0m, \u001b[33mis_fast\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mpadding_side\u001b[0m=\u001b[32m'right'\u001b[0m, \u001b[33mtruncation_side\u001b[0m=\u001b[32m'right'\u001b[0m, \n",
       "\u001b[33mspecial_tokens\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'bos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'eos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'unk_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \n",
       "\u001b[32m'pad_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'additional_special_tokens'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'### End'\u001b[0m, \u001b[32m'### Instruction:'\u001b[0m, \u001b[32m'### Response:'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m, \n",
       "\u001b[33mclean_up_tokenization_spaces\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Special Tokens\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "INPUT_KEY = \"Input:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "RESPONSE_KEY_NL = f\"{RESPONSE_KEY}\\n\"\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"databricks/dolly-v2-3b\"\n",
    "eleutherai_python_3b = \"EleutherAI/pythia-2.8b\"\n",
    "dolly_v2_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "print(dolly_v2_tokenizer)\n",
    "pythia_tokenizer = AutoTokenizer.from_pretrained(eleutherai_python_3b)\n",
    "print(pythia_tokenizer)\n",
    "\n",
    "# Make sure that the pad token is the end of the tokens\n",
    "pythia_tokenizer.pad_token = pythia_tokenizer.eos_token\n",
    "\n",
    "# Add special tokens for End , Instruction , Response Key\n",
    "\n",
    "pythia_tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\n",
    "        END_KEY,\n",
    "        INSTRUCTION_KEY,\n",
    "        RESPONSE_KEY,\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(pythia_tokenizer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Model\n",
    "1. Torch Datat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Bits and Butes Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neox-20b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold = 6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the LM Models\n",
    "Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "n_gpus = torch.cuda.device_count()\n",
    "\n",
    "free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path = PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "    trust_remote_code = True,\n",
    "    use_cache = False,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = \"auto\",\n",
    "    load_in_4bit = True,\n",
    "    load_in_8bit = False,\n",
    "    low_cpu_mem_usage = True,\n",
    "    max_memory =  max_memory,\n",
    "    quantize_config = bnb_config,\n",
    ")\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the LoRa Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] [Dolly Github](https://github.com/databrickslabs/dolly/blob/5021d941d95dddcf1f00d978d7f944709873f419/training/trainer.py#L138)\n",
    "[2] https://gist.github.com/Birch-san/57878c4a27cf34f57d3e861865a7d0a2\n",
    "[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temoctalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
