{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all the Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the copmuter is on google colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    !pip install rich\n",
    "    !pip install -q -U bitsandbytes\n",
    "    !pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "    !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "    !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "    !pip install datasets\n",
    "    !pip install wandb\n",
    "    !pip install ray[tune]\n",
    "else:\n",
    "    print(\"Not running on Google Colab\")\n",
    "from rich import print\n",
    "import logging\n",
    "from pathlib import Path\n",
    "logger = logging.getLogger(__name__)\n",
    "#ROOT_PATH = Path(__file__).parent.parent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the GPU env\n",
    "1. You can check the GPU in the Google Colab by clicking  and efficieny\n",
    "2. Check if the GPU can use bfloat16 most effective as most model are pre-trained with bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from rich import print\n",
    "if torch.cuda.is_available():\n",
    "    !nvidia-smi\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"Cuda capability: \", torch.cuda.get_device_capability(0))\n",
    "    '''\n",
    "    On pre-ampere hardware bf16 works, but doesn't provide speed-ups compared to fp32 matmul operations, and some matmul operations are failing outright, so this check is more like \"guaranteed to work and be performant\" than \"works somehow\".  https://github.com/pytorch/pytorch/issues/75427\n",
    "    '''\n",
    "    print(f\"bfloat16 support: { torch.cuda.is_bf16_supported()}\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Seed Environment of the Notebook to ensure the reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "set_seed( DEFAULT_SEED )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH  = 1024\n",
    "BITS  = 4\n",
    "LR = 1e-5\n",
    "EPOCHS = 2\n",
    "# Whether to use bf16 (preferred on A100's).\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Weight And Bias ðŸ’¡ Configuration tips\n",
    "\n",
    "W&B integration with Hugging Face can be configured to add extra functionalities:\n",
    "\n",
    "* auto-logging of models as artifacts: just set environment varilable `WANDB_LOG_MODEL` to `true`\n",
    "* log histograms of gradients and parameters: by default gradients are logged, you can also log parameters by setting environment variable `WANDB_WATCH` to `all`\n",
    "* set custom run names with `run_name` arg present in scripts or as part of `TrainingArguments`\n",
    "* organize runs by project with the `WANDB_PROJECT` environment variable\n",
    "\n",
    "For more details refer to [W&B + HF integration documentation](https://docs.wandb.ai/integrations/huggingface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "%env WANDB_LOG_MODEL=true\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"prompt_generator\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "#@os.environ[\"WANDB_LOG_MODEL\"]\n",
    "\n",
    "# turn off watch to log faster\n",
    "#os.environ[\"WANDB_WATCH\"]= \"all\" # I am getting an error with this\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Local Training Root"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Datset from the Hugging Face Datset Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TRAINING_DATASET = \"Rami/prompts\"\n",
    "from datasets import load_dataset\n",
    "training_dataset = load_dataset(\n",
    "    \"Rami/prompts\",\n",
    ")\n",
    "print(\n",
    "    training_dataset\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Tokenizers\n",
    "1. We are suing Dolly model which was trained on the Pythia model. Instead we are recreating the dollvy tokenizer from the Pythia tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Special Tokens\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "INPUT_KEY = \"Input:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "RESPONSE_KEY_NL = f\"{RESPONSE_KEY}\\n\"\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"databricks/dolly-v2-7b\"#\"databricks/dolly-v2-3b\"\n",
    "eleutherai_python_3b = \"EleutherAI/pythia-2.8b\"\n",
    "eleutherai_python_7b = \"EleutherAI/pythia-6.9b\"\n",
    "dolly_v2_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "print(dolly_v2_tokenizer)\n",
    "pythia_tokenizer = AutoTokenizer.from_pretrained(eleutherai_python_7b)\n",
    "print(pythia_tokenizer)\n",
    "\n",
    "# Make sure that the pad token is the end of the tokens\n",
    "pythia_tokenizer.pad_token = pythia_tokenizer.eos_token\n",
    "\n",
    "# Add special tokens for End , Instruction , Response Key\n",
    "pythia_tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\n",
    "        END_KEY,\n",
    "        INSTRUCTION_KEY,\n",
    "        RESPONSE_KEY,\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(pythia_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the dataset\n",
    "1. Convert the dataset into instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruct Format String and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTRO_BLURB = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    ")\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "INPUT_KEY = \"Input:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "RESPONSE_KEY_NL = f\"{RESPONSE_KEY}\\n\"\n",
    "DEFAULT_SEED = 42\n",
    "# This is a training prompt that does not contain an input string.  The instruction by itself has enough information\n",
    "# to respond.  For example, the instruction might ask for the year a historic figure was born.\n",
    "PROMPT_NO_INPUT_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{response_key}\n",
    "{response}\n",
    "\n",
    "{end_key}\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    "    response=\"{response}\",\n",
    "    end_key=END_KEY,\n",
    ")\n",
    "\n",
    "# This is a training prompt that contains an input string that serves as context for the instruction.  For example,\n",
    "# the input might be a passage from Wikipedia and the intruction is to extract some information from it.\n",
    "PROMPT_WITH_INPUT_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{input_key}\n",
    "{input}\n",
    "\n",
    "{response_key}\n",
    "{response}\n",
    "\n",
    "{end_key}\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    input_key=INPUT_KEY,\n",
    "    input=\"{input}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    "    response=\"{response}\",\n",
    "    end_key=END_KEY,\n",
    ")\n",
    "\n",
    "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
    "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "def load_training_dataset(path_or_dataset: str = DEFAULT_TRAINING_DATASET) -> Dataset:\n",
    "    logger.info(f\"Loading dataset from {path_or_dataset}\")\n",
    "    dataset = load_dataset(path_or_dataset)[\"train\"]\n",
    "    logger.info(\"Found %d rows\", dataset.num_rows)\n",
    "\n",
    "    def _add_text(rec):\n",
    "        instruction = rec[\"instruction\"]\n",
    "        response = rec[\"response\"]\n",
    "        context = rec.get(\"context\")\n",
    "\n",
    "        if not instruction:\n",
    "            raise ValueError(f\"Expected an instruction in: {rec}\")\n",
    "\n",
    "        if not response:\n",
    "            raise ValueError(f\"Expected a response in: {rec}\")\n",
    "\n",
    "        # For some instructions there is an input that goes along with the instruction, providing context for the\n",
    "        # instruction.  For example, the input might be a passage from Wikipedia and the instruction says to extract\n",
    "        # some piece of information from it.  The response is that information to extract.  In other cases there is\n",
    "        # no input.  For example, the instruction might be open QA such as asking what year some historic figure was\n",
    "        # born.\n",
    "        if context:\n",
    "            rec[\"text\"] = PROMPT_WITH_INPUT_FORMAT.format(instruction=instruction, response=response, input=context)\n",
    "        else:\n",
    "            rec[\"text\"] = PROMPT_NO_INPUT_FORMAT.format(instruction=instruction, response=response)\n",
    "        return rec\n",
    "\n",
    "    dataset = dataset.map(_add_text)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the dataset using the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    PreTrainedTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "def preprocess_batch(batch: Dict[str, List], tokenizer: AutoTokenizer, max_length: int) -> dict:\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed=DEFAULT_SEED, training_dataset: str = DEFAULT_TRAINING_DATASET) -> Dataset:\n",
    "    \"\"\"Loads the training dataset and tokenizes it so it is ready for training.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (AutoTokenizer): Tokenizer tied to the model.\n",
    "        max_length (int): Maximum number of tokens to emit from tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: HuggingFace dataset\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = load_training_dataset(training_dataset)\n",
    "\n",
    "    logger.info(\"Preprocessing dataset\")\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "    )\n",
    "\n",
    "    # Make sure we don't have any truncated records, as this would mean the end keyword is missing.\n",
    "    logger.info(\"Processed dataset has %d rows\", dataset.num_rows)\n",
    "    dataset = dataset.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)\n",
    "    logger.info(\"Processed dataset has %d rows after filtering for truncated records\", dataset.num_rows)\n",
    "\n",
    "    logger.info(\"Shuffling dataset\")\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    logger.info(\"Done preprocessing\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = preprocess_dataset(\n",
    "    tokenizer = pythia_tokenizer,\n",
    "    max_length = MAX_LENGTH,\n",
    "    seed = DEFAULT_SEED,\n",
    ")\n",
    "\n",
    "# Split the Dataset\n",
    "\n",
    "split_dataset = processed_dataset.train_test_split(test_size = 0.2, seed=DEFAULT_SEED, shuffle = True)\n",
    "\n",
    "train_datset = split_dataset[\"train\"]\n",
    "print(train_datset)\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "logger.info(\"Train data size: %d\", split_dataset[\"train\"].num_rows)\n",
    "logger.info(\"Test data size: %d\", split_dataset[\"test\"].num_rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator For Completion Only LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class DataCollatorForCompletionOnlyLM(DataCollatorForLanguageModeling):\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        # The prompt ends with the response key plus a newline.  We encode this and then try to find it in the\n",
    "        # sequence of tokens.  This should just be a single token.\n",
    "        response_token_ids = self.tokenizer.encode(RESPONSE_KEY_NL)\n",
    "\n",
    "        labels = batch[\"labels\"].clone()\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "\n",
    "            response_token_ids_start_idx = None\n",
    "            for idx in np.where(batch[\"labels\"][i] == response_token_ids[0])[0]:\n",
    "                response_token_ids_start_idx = idx\n",
    "                break\n",
    "\n",
    "            if response_token_ids_start_idx is None:\n",
    "                raise RuntimeError(\n",
    "                    f'Could not find response key {response_token_ids} in token IDs {batch[\"labels\"][i]}'\n",
    "                )\n",
    "\n",
    "            response_token_ids_end_idx = response_token_ids_start_idx + 1\n",
    "\n",
    "            # Make pytorch loss function ignore all tokens up through the end of the response key\n",
    "            labels[i, :response_token_ids_end_idx] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"A100\" in torch.cuda.get_device_name(0):\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True \n",
    "else:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = False\n",
    "# https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer = pythia_tokenizer,\n",
    "    mlm=False, \n",
    "    return_tensors=\"pt\", \n",
    "    pad_to_multiple_of =   64 if torch.cuda.get_device_name(0) in [\"A100-SXM4-40GB\", \"A100-SXM4-80GB\"] else 8\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Model\n",
    "1. Torch Datat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Bits and Butes Config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Bit Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neox-20b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit = False,\n",
    "    llm_int8_threshold = 6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the LM Models\n",
    "Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "assert torch.cuda.is_available(), \"You need to have a GPU to run this notebook.\"\n",
    "n_gpus = torch.cuda.device_count()\n",
    "def model_int():\n",
    "    free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "    max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path = PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "        trust_remote_code = True,\n",
    "        use_cache = False,\n",
    "        torch_dtype =  torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        device_map = \"auto\",\n",
    "        load_in_8bit = False,\n",
    "        load_in_4bit = True,\n",
    "        low_cpu_mem_usage = True,\n",
    "        max_memory =  max_memory,\n",
    "        quantization_config = bnb_config,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = model_int()\n",
    "\n",
    "print(model.vocab_size)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "def print_trainable_parameters(args, model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    if args.bits == 4: trainable_params /= 2\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || \"\n",
    "        f\"all params: {all_param} || \"\n",
    "        f\"trainable: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "try:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "except:\n",
    "    print(\"Model is already prepared for kbit training\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the LoRa Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import bitsandbytes as bnb\n",
    "def find_all_linear_names( model):\n",
    "    cls = bnb.nn.Linear4bit if BITS == 4 else (bnb.nn.Linear8bitLt if BITS == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules = find_all_linear_names(  model),\n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "try:\n",
    "    print_trainable_parameters(model)\n",
    "except:\n",
    "    print(\"Model is already prepared for kbit training\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import join\n",
    "from os.path import exists, join, isdir\n",
    "import transformers\n",
    "class SavePeftModelCallback(transformers.TrainerCallback):\n",
    "    def save_model(self, args, state, kwargs):\n",
    "        print('Saving PEFT checkpoint...')\n",
    "        if state.best_model_checkpoint is not None:\n",
    "            checkpoint_folder = os.path.join(state.best_model_checkpoint, \"adapter_model\")\n",
    "        else:\n",
    "            checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        self.save_model(args, state, kwargs)\n",
    "        return control\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        def touch(fname, times=None):\n",
    "            with open(fname, 'a'):\n",
    "                os.utime(fname, times)\n",
    "\n",
    "        touch(join(args.output_dir, 'completed'))\n",
    "        self.save_model(args, state, kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Arguments \n",
    "1. WarmUp Steps \n",
    "2. Learning Rate \n",
    "   1.  0.0001 <path_or_name> https://github.com/artidoro/qlora/tree/main if the model is bigger than 13 Billions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments , EarlyStoppingCallback\n",
    "batch_size: int = 64 if \"NVIDIA A100-SXM4-40GB\" in torch.cuda.get_device_name(0) else 8 # if the model is too large for your GPU, decrease this number\n",
    "micro_batch_size: int = 4\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "learning_rate: float = 0.0002\n",
    "epochs = 4\n",
    "print(f\"learning_rate: {learning_rate}\")\n",
    "trainer_arguments = TrainingArguments(\n",
    "            output_dir = \"outputs\",\n",
    "            per_device_train_batch_size = batch_size,\n",
    "            per_device_eval_batch_size = batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            num_train_epochs = epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            lr_scheduler_type = \"linear\",\n",
    "            warmup_ratio= .3,\n",
    "            bf16 =  True if torch.cuda.is_bf16_supported() else False,\n",
    "            fp16 =   False if  torch.cuda.is_bf16_supported() else True,\n",
    "            logging_steps = 20, # we will log every 20 steps\n",
    "            eval_steps = 20, # we will evaluate every 20 steps\n",
    "            evaluation_strategy=\"steps\",\n",
    "            optim = \"paged_adamw_8bit\",\n",
    "            save_strategy=\"steps\",\n",
    "            report_to = [\"wandb\", \"tensorboard\"],\n",
    "            load_best_model_at_end = True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            seed = DEFAULT_SEED,\n",
    "            run_name = \"dolly-v2-7b-prompt-generator-paged-adamw-8bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training run 0\n",
      "1.0\n",
      "Training run 1\n",
      "2.0\n",
      "Training run 2\n",
      "4.0\n",
      "Training run 3\n",
      "8.0\n",
      "Training run 4\n",
      "16.0\n",
      "Training run 5\n",
      "32.0\n",
      "Training run 6\n",
      "64.0\n",
      "Training run 7\n",
      "128.0\n",
      "Training run 8\n",
      "256.0\n",
      "Training run 9\n",
      "512.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for index in range(10):\n",
    "    print(f\"Training run {index}\")\n",
    "    #power of two\n",
    "    print(math.pow(2, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer = pythia_tokenizer,\n",
    "    args = trainer_arguments,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [ EarlyStoppingCallback(\n",
    "        early_stopping_patience = 0 , \n",
    "        )\n",
    "                 ],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logger.info(\"Training\")\n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = trainer.state.log_history\n",
    "print(train_logs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push the Model to the Hugging Face Model Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"Rami/dolly_prompt_generator\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_int():\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] [Dolly Github](https://github.com/databrickslabs/dolly/blob/5021d941d95dddcf1f00d978d7f944709873f419/training/trainer.py#L138)\n",
    "[2] https://gist.github.com/Birch-san/57878c4a27cf34f57d3e861865a7d0a2\n",
    "[3] https://github.com/artidoro/qlora/blob/main/qlora.py \n",
    "[4] https://github.com/tloen/alpaca-lora/blob/main/finetune.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temoctalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
